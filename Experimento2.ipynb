{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenisPerez/NN_Tests_DG/blob/main/Experimento2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimento 2: Métodos con Momentum"
      ],
      "metadata": {
        "id": "SqxJ-9htEHBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Propósito:** Estudiar el desempeño y la velocidad de convergencia basado en epochs de los métodos cíclicos y el método decreciente (propio) combinados con la estrategia de Momentum\n",
        "\n",
        "\n",
        "> **Nota:** Si se utilizará Google Colab como ambiente para las pruebas, se debe tomar la referencia al repositorio para que la libreta tenga acceso a los archivos que requiere\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nwo9P_NCEZNE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBCruOrtR_cV",
        "outputId": "b2ea0ab9-26de-4ef2-daae-cbcee6c12614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NN_Tests_DG'...\n",
            "remote: Enumerating objects: 90, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 90 (delta 38), reused 52 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (90/90), 15.22 MiB | 6.66 MiB/s, done.\n",
            "/content/NN_Tests_DG/NN_Tests_DG/NN_Tests_DG/NN_Tests_DG/NN_Tests_DG\n",
            "classes.py          get_images.py             LearningRateStudy.ipynb\n",
            "\u001b[0m\u001b[01;34mDataset\u001b[0m/            Iteracion1.ipynb          NN_from_Scratch.ipynb\n",
            "Decay.ipynb         Iteracion1_new.ipynb      NN_Pytorch.ipynb\n",
            "Experimento2.ipynb  Iteracion_2.ipynb         ResNet56_cifar10.ipynb\n",
            "Experimento3.ipynb  Iteracion3_Mejores.ipynb  SGD_Momentum_RMSProp_Adam.ipynb\n"
          ]
        }
      ],
      "source": [
        "!git clone https://ginpg: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@github.com/DenisPerez/NN_Tests_DG\n",
        "%cd NN_Tests_DG\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al tener disponible los archivos a referenciar, se realizan las importaciones necesarias"
      ],
      "metadata": {
        "id": "Ez6NV8kTFfF6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "F4gP7LPQTtg7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from get_images import get_images\n",
        "from classes import CyclicLRGiselt_Denis\n",
        "\n",
        "# PyTorch \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-mULGvWTthB"
      },
      "source": [
        "### Extracción del conjunto de datos: MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este conjundo de datos esta basado en 60.000 imá-\n",
        "genes en el conjunto de entrenamiento y 10.000 en el conjunto de prueba de 28x28 píxeles\n",
        "que representan dígitos del 0 al 9 escritos a mano y es considerado el \"Hola Mundo\" en el\n",
        "área de la ciencia de datos. Sin embargo, **10.000  de las muestras del conjunto de entrenamiento serán destinadas al conjunto de validación** con el que se te tomará la precisión en el entrenamiento."
      ],
      "metadata": {
        "id": "oFhRucJEF8OJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "9vdIQd58TthD"
      },
      "outputs": [],
      "source": [
        "MNIST_PATH = './Dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "slQPSqQeTthE"
      },
      "outputs": [],
      "source": [
        "x_train_num, y_train_num, x_test_num, y_test_num = get_images(MNIST_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "HFnaRwd-TthE"
      },
      "outputs": [],
      "source": [
        "# Segmento para el conjunto de entrenamiento\n",
        "x_train = x_train_num[:50000].reshape(50000,-1).astype(np.float32)/255 ##Convert the traint set into a (50000, 28x28) matrix normalized\n",
        "y_train = y_train_num[:50000].reshape(50000,1)\n",
        "\n",
        "# Segmento para el conjunto de validacion\n",
        "x_val = x_train_num[50000:].reshape(10000,-1).astype(np.float32)/255\n",
        "y_val = y_train_num[50000:].reshape(10000,1)\n",
        "\n",
        "# Segmento para el conjunto de prueba\n",
        "x_test = x_test_num.copy().reshape(10000,-1).astype(np.float32)/255\n",
        "y_test = y_test_num.copy().reshape(10000,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZz8hG0OTthF"
      },
      "source": [
        "### Normalización de la muestra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "_kFtDZs6TthG"
      },
      "outputs": [],
      "source": [
        "def normalize(x_mean, x_std, x_data):\n",
        "    return (x_data - x_mean) / x_std"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se toma la desviación estandar y la media de cada conjunto de datos y se llama a la función *normalize*"
      ],
      "metadata": {
        "id": "hMivLxHhGRdK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "KYhJenIZTthG"
      },
      "outputs": [],
      "source": [
        "# Calculo para el conjunto de entrenamiento\n",
        "x_mean = x_train.mean()\n",
        "x_std = x_train.std()\n",
        "x_train = normalize(x_mean, x_std, x_train)\n",
        "\n",
        "# Calculo para el conjunto de validación\n",
        "x_mean = x_val.mean()\n",
        "x_std = x_val.std()\n",
        "x_val = normalize(x_mean, x_std, x_val)\n",
        "\n",
        "# Calculo para el conjunto de prueba\n",
        "x_mean = x_test.mean()\n",
        "x_std = x_test.std()\n",
        "x_test = normalize(x_mean, x_std, x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZfqsORc6GMUN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMAv3EVXTthJ"
      },
      "source": [
        "### Visualización de la muestra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHHAe687TthJ",
        "outputId": "dd159823-25ab-4649-eeb0-644ad46823cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ],
      "source": [
        "# Dimensiones del conjunto de datos de entrenamiento\n",
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_FPmhofTthK",
        "outputId": "8c070479-5564-4907-d9f3-7a1b5c7b3980"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ],
      "source": [
        "# Dimensiones del conjunto de datos de prueba:\n",
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "w4HfkSTqTthK"
      },
      "outputs": [],
      "source": [
        "def plot_number(image):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "7TQtWTpZTthL",
        "outputId": "d13e1529-a79d-4d45-b2f5-fe07f115e0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La imagen muestreada representa un: 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "rnd_idx = np.random.randint(len(y_test))\n",
        "print(f'La imagen muestreada representa un: {y_test[rnd_idx, 0]}')\n",
        "plot_number(x_test_num[rnd_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVo-PF6cTthM"
      },
      "source": [
        "### Creación de mini lotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "Ym639R_PTthM"
      },
      "outputs": [],
      "source": [
        "def create_minibatches(x, y, mb_size, shuffle = True):\n",
        "    assert x.shape[0] == y.shape[0], 'Error en cantidad de muestras'\n",
        "    total_data = x.shape[0]\n",
        "    if shuffle: \n",
        "        idxs = np.arange(total_data, dtype=float)\n",
        "        np.random.shuffle(idxs)\n",
        "        x = x[idxs]\n",
        "        y = y[idxs]  \n",
        "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5Ypqe6nTthN"
      },
      "source": [
        "### Conversión de arreglo a tensores para todos los conjuntos de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "6quE_eQ9TthO"
      },
      "outputs": [],
      "source": [
        "# Conversion para el conjunto de entrenamiento\n",
        "x_train_tensor = torch.Tensor(x_train.copy())\n",
        "y_train_tensor = torch.Tensor(y_train.copy())\n",
        "\n",
        "# Conversion para el conjunto de validación\n",
        "x_val_tensor = torch.Tensor(x_val.copy())\n",
        "y_val_tensor = torch.Tensor(y_val.copy())\n",
        "\n",
        "# Conversion para el conjunto de prueba\n",
        "x_test_tensor = torch.Tensor(x_test.copy())\n",
        "y_test_tensor = torch.Tensor(y_test.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqPgXB1DTthP"
      },
      "source": [
        "### Habilitar el uso del CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero se consulta si se tiene la plataforma CUDA disponible para la utilización de los recursos de GPU"
      ],
      "metadata": {
        "id": "StMO7lKcNcfd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1oc0PTWTthQ",
        "outputId": "d74485f7-f9f4-4fdb-fa79-38e14f118f8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En caso de no tenerse, se asigna el trabajo de computo al CPU"
      ],
      "metadata": {
        "id": "jStmz-iVODwZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZebY-iBHTthQ",
        "outputId": "6b4de3de-6d2e-44d6-a62b-4f3c6f2c13b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estamos usando: cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(f'Estamos usando: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L66abJ9WTthR"
      },
      "source": [
        "# Funciones "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función para calcular la precisión"
      ],
      "metadata": {
        "id": "4k4fCzPrQrke"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "Zt6iiZyuTthR"
      },
      "outputs": [],
      "source": [
        "def accuracy(model: nn.Sequential, x: torch.tensor, y: torch.tensor, mb_size: int):\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    model.eval()\n",
        "    model = model.to(device=device)\n",
        "    with torch.no_grad():\n",
        "        for (xi, yi) in create_minibatches(x, y, mb_size):\n",
        "            xi = xi.to(device=device, dtype = torch.float32)\n",
        "            yi = yi.to(device=device, dtype = torch.long)\n",
        "            scores = model(xi) # mb_size, 10\n",
        "            _, pred = scores.max(dim=1) #pred shape (mb_size )\n",
        "            num_correct += (pred == yi.squeeze()).sum() # pred shape (mb_size), yi shape (mb_size, 1)\n",
        "            num_total += pred.size(0)\n",
        "\n",
        "            return float(num_correct)/num_total  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR4DuKf3TthR"
      },
      "source": [
        "### Función para realizar la propagación hacia atrás y la actualización de parámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "HtRDD6Nu0507"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, scheduler: None, mb_size):\n",
        "    model = model.to(device=device)\n",
        "    epoch_acc = 0.0\n",
        "    i = 0\n",
        "    #plot lists\n",
        "    acc_list = [0.0]\n",
        "    cost_list = [0.0]\n",
        "    lr_list = [0.0]\n",
        "    while (i < 100):\n",
        "        if (epoch_acc >= 0.95):\n",
        "          iter_found = i\n",
        "          break\n",
        "        for (xi, yi) in create_minibatches(x_train_tensor, y_train_tensor, mb_size):\n",
        "            model.train()\n",
        "            xi = xi.to(device=device, dtype=torch.float32)\n",
        "            yi = yi.to(device=device, dtype=torch.long)\n",
        "            scores = model(xi)\n",
        "            # cost function\n",
        "            cost = F.cross_entropy(input= scores, target=yi.squeeze())\n",
        "            optimizer.zero_grad()\n",
        "            cost.backward()\n",
        "            optimizer.step()\n",
        "        if (scheduler != None):\n",
        "          scheduler.step()\n",
        "          lr = scheduler.get_last_lr()\n",
        "          lr_list.append(lr[0])\n",
        "        else: \n",
        "          lr_list.append(optimizer.param_groups[0]['lr'])\n",
        "        i+=1\n",
        "        epoch_acc = accuracy(model, x_val_tensor, y_val_tensor, mb_size)\n",
        "        epoch_cost = cost.item()\n",
        "\n",
        "        #append\n",
        "        acc_list.append(epoch_acc)\n",
        "        cost_list.append(epoch_cost)\n",
        "        # print(f'Epoch: {len(acc_list)}, learning_rate:{lr},costo: {epoch_cost}, accuracy: {epoch_acc}')\n",
        "    return acc_list, cost_list, lr_list, iter_found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6xxVksqR95C"
      },
      "source": [
        "### Operaciones en las trazas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "5aJV7jOTR95D"
      },
      "outputs": [],
      "source": [
        "def SumList(first: list, second: list) -> list:\n",
        "    return [x + y for x, y in zip(first[::-1], second[::-1])][::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "I51hZdnHR95D"
      },
      "outputs": [],
      "source": [
        "def DivideList(dic_list: list, number: int) -> list:\n",
        "    return [x / number for x in dic_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "y6ro6TQRR95E"
      },
      "outputs": [],
      "source": [
        "def DeleteZerosFromList(dic_list: list) -> list:\n",
        "    return list(filter(lambda num: num != 0, dic_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "015SGTIaTthS"
      },
      "source": [
        "# Experimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ2qa3c-eS1L"
      },
      "source": [
        "### Variables globales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "WBDuey1VQTm4"
      },
      "outputs": [],
      "source": [
        "MAX_ITERATIONS = 1\n",
        "layer1 = 1000 \n",
        "layer2 = 1000\n",
        "lr = 1e-2\n",
        "epochs = 100\n",
        "mb_size = 4096\n",
        "input_layer = 784\n",
        "first_i = 0\n",
        "dropout = 0.25\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "resultados = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mAWn11Vmc6-"
      },
      "source": [
        "## Tasa de aprendizaje cíclica"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oscila entre una $base\\_ta$ y un $max\\_ta$ cada $n$ epochs en una proporción igual a \n",
        "\n",
        "$$\n",
        "proporción\\_de\\_cambio = \\frac{max\\_ta - base\\_ta}{n}\n",
        "$$\n",
        "\n",
        "Esta implementación facilitada por PyTorch, está basada en la publicación de Tasas de aprendizaje cíclico para la red neuronal de entrenamiento (*Cyclical Learning Rates for Training Neural Networks* o por sus siglas CLR) de Leslie Smith, 2017.\n",
        "\n",
        "$\\alpha: 1x10^{-2}$\n",
        "\n",
        " $beta: 0.9$"
      ],
      "metadata": {
        "id": "6zZoJLD7cQID"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "_DdXVK_ZR95I"
      },
      "outputs": [],
      "source": [
        "def Cyclic():\n",
        "    modelCyclic = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1),  nn.ReLU(),\n",
        "                        nn.Linear(in_features=layer1, out_features=layer2),  nn.ReLU(),\n",
        "                        nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelCyclic.parameters(), lr=lr)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1,step_size_up=3,cycle_momentum=False )\n",
        "\n",
        "    start.record()\n",
        "    cyclic_acc_list, cyclic_cost_list, cyclic_lr_list, cyclic_epochs= train(modelCyclic, optimizer,scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    cyclic_time = start.elapsed_time(end)\n",
        "\n",
        "    cyclic_acc = accuracy(modelCyclic, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return cyclic_acc_list, cyclic_cost_list, cyclic_lr_list, cyclic_time, cyclic_acc, cyclic_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "qWp659FGmhDK"
      },
      "outputs": [],
      "source": [
        "\n",
        "resultados['cyclic'] = {}\n",
        "resultados['cyclic']['val_acc_list'] = [0]* epochs\n",
        "resultados['cyclic']['test_acc'] = 0\n",
        "resultados['cyclic']['cost'] = [0] * epochs\n",
        "resultados['cyclic']['time'] = 0\n",
        "resultados['cyclic']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    cyclic_acc_list, cyclic_cost_list, cyclic_lr_list, cyclic_time, cyclic_acc, cyclic_epochs = Cyclic()\n",
        "    resultados['cyclic']['val_acc_list'] = SumList(resultados['cyclic']['val_acc_list'], cyclic_acc_list)\n",
        "    resultados['cyclic']['test_acc'] += cyclic_acc\n",
        "    resultados['cyclic']['cost'] = SumList(resultados['cyclic']['cost'], cyclic_cost_list)\n",
        "    resultados['cyclic']['time'] += cyclic_time\n",
        "    resultados['cyclic']['epochs'] += cyclic_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['cyclic']['name'] = 'Ciclico'\n",
        "resultados['cyclic']['lr'] = cyclic_lr_list\n",
        "resultados['cyclic']['test_acc'] = resultados['cyclic']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['cyclic']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['cyclic']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['cyclic']['cost'] = DeleteZerosFromList(DivideList(resultados['cyclic']['cost'], MAX_ITERATIONS))\n",
        "resultados['cyclic']['time'] = resultados['cyclic']['time']/MAX_ITERATIONS\n",
        "resultados['cyclic']['epochs'] = resultados['cyclic']['epochs']/MAX_ITERATIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f8ZZZ1Hcg2R"
      },
      "source": [
        "## Tasa de aprendizaje cíclica aleatoria (propuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oscila entre una $base\\_ta$ y un $max\\_ta$ cada $n$ epochs en una proporción aleatoria.\n",
        "\n",
        "$base\\_ta: 1x10^{-2}$\n",
        "\n",
        "$max\\_ta: 1x10^{-1}$\n",
        "\n",
        "$n: 3$ epochs"
      ],
      "metadata": {
        "id": "aHZHdKoxeQZp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "jywqBaWVR95N"
      },
      "outputs": [],
      "source": [
        "def CyclicGD():\n",
        "    modelRandomCyclic = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1),  nn.ReLU(),\n",
        "                        nn.Linear(in_features=layer1, out_features=layer2),  nn.ReLU(),\n",
        "                        nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelRandomCyclic.parameters(), lr=lr)\n",
        "\n",
        "    clr_fn = lambda x, y: random.uniform(x, y)\n",
        "\n",
        "    scheduler = CyclicLRGiselt_Denis(optimizer, base_lr=0.01, max_lr=0.1,step_size_up=3, scale_fn=clr_fn,scale_mode='chipichipi', cycle_momentum=False)\n",
        "\n",
        "    start.record()\n",
        "    random_cyclic_acc_list, random_cyclic_cost_list, random_cyclic_lr_list, random_cyclic_epochs= train(modelRandomCyclic, optimizer,scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    random_cyclic_time = start.elapsed_time(end)\n",
        "\n",
        "    random_cyclic_acc = accuracy(modelRandomCyclic, x_test_tensor,  y_test_tensor, mb_size) \n",
        "\n",
        "    return random_cyclic_acc_list, random_cyclic_cost_list, random_cyclic_lr_list, random_cyclic_time, random_cyclic_acc, random_cyclic_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "acBgxgGkZdL6"
      },
      "outputs": [],
      "source": [
        "\n",
        "resultados['random_cyclic'] = {}\n",
        "resultados['random_cyclic']['val_acc_list'] = [0] * epochs\n",
        "resultados['random_cyclic']['test_acc'] = 0\n",
        "resultados['random_cyclic']['cost'] = [0] * epochs\n",
        "resultados['random_cyclic']['time'] = 0\n",
        "resultados['random_cyclic']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    random_cyclic_acc_list, random_cyclic_cost_list, random_cyclic_lr_list, random_cyclic_time, random_cyclic_acc, random_cyclic_epochs = CyclicGD()\n",
        "    a = SumList(resultados['random_cyclic']['val_acc_list'], random_cyclic_acc_list)\n",
        "    resultados['random_cyclic']['val_acc_list'] = SumList(resultados['random_cyclic']['val_acc_list'], random_cyclic_acc_list)\n",
        "    resultados['random_cyclic']['test_acc'] += random_cyclic_acc\n",
        "    resultados['random_cyclic']['cost'] = SumList(resultados['random_cyclic']['cost'], random_cyclic_cost_list)\n",
        "    resultados['random_cyclic']['time'] += random_cyclic_time\n",
        "    resultados['random_cyclic']['epochs'] += random_cyclic_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['random_cyclic']['name'] = 'Random Ciclico'\n",
        "resultados['random_cyclic']['lr'] = random_cyclic_lr_list\n",
        "resultados['random_cyclic']['test_acc'] = resultados['random_cyclic']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['random_cyclic']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['random_cyclic']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['random_cyclic']['cost'] = DeleteZerosFromList(DivideList(resultados['random_cyclic']['cost'], MAX_ITERATIONS))\n",
        "resultados['random_cyclic']['time'] = resultados['cyclic']['time']/MAX_ITERATIONS\n",
        "resultados['random_cyclic']['epochs'] = resultados['cyclic']['epochs']/MAX_ITERATIONS\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzehV-6emhDY"
      },
      "source": [
        "## Momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se utiliza una optimización por el descenso del gradiente estocástico empleando Momentum.\n",
        "\n",
        "$beta: 0.9$"
      ],
      "metadata": {
        "id": "5COVE7Tqe4n8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "Bt6bkSegR95R"
      },
      "outputs": [],
      "source": [
        "def SGDM():\n",
        "    modelSGDM = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1),  nn.ReLU(),\n",
        "                              nn.Linear(in_features=layer1, out_features=layer2),  nn.ReLU(),\n",
        "                              nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimiserSGDM = torch.optim.SGD(modelSGDM.parameters(), lr=lr, momentum=0.9)\n",
        "    start.record()\n",
        "    SGDM_acc_list, SGDM_cost_list,SGDM_lr_list, SGDM_epochs = train(modelSGDM, optimiserSGDM,None, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    SGDM_time = start.elapsed_time(end)\n",
        "\n",
        "    SGDM_acc = accuracy(modelSGDM, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return SGDM_acc_list, SGDM_cost_list, SGDM_lr_list, SGDM_time, SGDM_acc, SGDM_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "U4bdgJL-mkx3"
      },
      "outputs": [],
      "source": [
        "\n",
        "resultados['SGDM'] = {}\n",
        "resultados['SGDM']['val_acc_list'] = [0] * epochs\n",
        "resultados['SGDM']['test_acc'] = 0\n",
        "resultados['SGDM']['cost'] = [0] * epochs\n",
        "resultados['SGDM']['time'] = 0\n",
        "resultados['SGDM']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    SGDM_acc_list, SGDM_cost_list, SGDM_lr_list, SGDM_time, SGDM_acc, SGDM_epochs = SGDM()\n",
        "    resultados['SGDM']['val_acc_list'] = SumList(resultados['SGDM']['val_acc_list'], SGDM_acc_list)\n",
        "    resultados['SGDM']['test_acc'] += SGDM_acc\n",
        "    resultados['SGDM']['cost'] = SumList(resultados['SGDM']['cost'], SGDM_cost_list)\n",
        "    resultados['SGDM']['time'] += SGDM_time\n",
        "    resultados['SGDM']['epochs'] += SGDM_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['SGDM']['name'] = 'Momentum'\n",
        "resultados['SGDM']['lr'] = SGDM_lr_list\n",
        "resultados['SGDM']['test_acc'] = resultados['SGDM']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['SGDM']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['SGDM']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['SGDM']['cost'] = DeleteZerosFromList(DivideList(resultados['SGDM']['cost'], MAX_ITERATIONS))\n",
        "resultados['SGDM']['time'] = resultados['SGDM']['time']/ MAX_ITERATIONS\n",
        "resultados['SGDM']['epochs'] = resultados['SGDM']['epochs']/ MAX_ITERATIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasa de aprendizaje decreciente (propuesta)"
      ],
      "metadata": {
        "id": "VWVaAGYywZbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicia con una tasa de aprendizaje $\\alpha$ inicial y posee una disminución de 0.001 cada $p$ epochs.\n",
        "\n",
        "$\\alpha: 1x10^{-2}$\n",
        "\n",
        "$p: 1$ epochs"
      ],
      "metadata": {
        "id": "7vTm7O1Wff3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Our_Decay():\n",
        "    modelOurDecay = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1),  nn.ReLU(),\n",
        "                              nn.Linear(in_features=layer1, out_features=layer2),  nn.ReLU(),\n",
        "                              nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelOurDecay.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "    clr_fn = lambda x, y: random.uniform(x, y)\n",
        "    scheduler = CyclicLRGiselt_Denis(optimizer, base_lr=0.0001, max_lr=0.1,scale_fn=clr_fn, step_size_up=1,scale_mode='decrecimiento', cycle_momentum=False)\n",
        "\n",
        "    start.record()\n",
        "    our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_epochs = train(modelOurDecay,optimizer, scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    our_decay_time = start.elapsed_time(end)\n",
        "\n",
        "    our_decay_acc = accuracy(modelOurDecay, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_time, our_decay_acc, our_decay_epochs"
      ],
      "metadata": {
        "id": "874_mQHJwb3X"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "resultados['our_decay'] = {}\n",
        "resultados['our_decay']['val_acc_list'] = [0] * epochs\n",
        "resultados['our_decay']['test_acc'] = 0\n",
        "resultados['our_decay']['cost'] = [0] * epochs\n",
        "resultados['our_decay']['time'] = 0\n",
        "resultados['our_decay']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_time, our_decay_acc, our_decay_epochs = Our_Decay()\n",
        "    resultados['our_decay']['val_acc_list'] = SumList(resultados['our_decay']['val_acc_list'], our_decay_acc_list)\n",
        "    resultados['our_decay']['test_acc'] += our_decay_acc\n",
        "    resultados['our_decay']['cost'] = SumList(resultados['our_decay']['cost'], our_decay_cost_list)\n",
        "    resultados['our_decay']['time'] += our_decay_time\n",
        "    resultados['our_decay']['epochs'] += our_decay_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['our_decay']['name'] = 'Nuestro decreciente'\n",
        "resultados['our_decay']['lr'] = our_decay_lr_list\n",
        "resultados['our_decay']['test_acc'] = resultados['our_decay']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['our_decay']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['our_decay']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['our_decay']['cost'] = DeleteZerosFromList(DivideList(resultados['our_decay']['cost'], MAX_ITERATIONS))\n",
        "resultados['our_decay']['time'] = resultados['our_decay']['time']  / MAX_ITERATIONS\n",
        "resultados['our_decay']['epochs'] = resultados['our_decay']['epochs'] / MAX_ITERATIONS\n"
      ],
      "metadata": {
        "id": "9f3dwRmBwe-2"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy_ipT4cV40p"
      },
      "source": [
        "## Tasa de aprendizaje cíclica con Momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$base\\_ta: 1x10^{-2}$\n",
        "\n",
        "$max\\_ta: 1x10^{-1}$\n",
        "\n",
        "$n: 3$ epochs\n",
        "\n",
        "$beta: 0.9$"
      ],
      "metadata": {
        "id": "MLmqBsF8hBCz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "z_1Gg-3eWMZr"
      },
      "outputs": [],
      "source": [
        "def CyclicMomentum():\n",
        "    modelCyclicMomentum = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                              nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                              nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelCyclicMomentum.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1,step_size_up=3,cycle_momentum=False )\n",
        "\n",
        "    start.record()\n",
        "    cyclicMomentum_acc_list, cyclicMomentum_cost_list, cyclicMomentum_lr_list, cyclicMomentum_epochs = train(modelCyclicMomentum, optimizer,scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    cyclicMomentum_time = start.elapsed_time(end)\n",
        "\n",
        "    cyclicMomentum_acc = accuracy(modelCyclicMomentum, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return cyclicMomentum_acc_list, cyclicMomentum_cost_list, cyclicMomentum_lr_list, cyclicMomentum_time, cyclicMomentum_acc, cyclicMomentum_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "GPA1FfOOWOtV"
      },
      "outputs": [],
      "source": [
        "resultados['cyclicMomentum'] = {}\n",
        "resultados['cyclicMomentum']['val_acc_list'] = [0]* epochs\n",
        "resultados['cyclicMomentum']['test_acc'] = 0\n",
        "resultados['cyclicMomentum']['cost'] = [0] * epochs\n",
        "resultados['cyclicMomentum']['time'] = 0\n",
        "resultados['cyclicMomentum']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    cyclicMomentum_acc_list, cyclicMomentum_cost_list, cyclicMomentum_lr_list, cyclicMomentum_time, cyclicMomentum_acc, cyclicMomentum_epochs= CyclicMomentum()\n",
        "    resultados['cyclicMomentum']['val_acc_list'] = SumList(resultados['cyclicMomentum']['val_acc_list'], cyclicMomentum_acc_list)\n",
        "    resultados['cyclicMomentum']['test_acc'] += cyclicMomentum_acc\n",
        "    resultados['cyclicMomentum']['cost'] = SumList(resultados['cyclicMomentum']['cost'], cyclicMomentum_cost_list)\n",
        "    resultados['cyclicMomentum']['time'] += cyclicMomentum_time\n",
        "    resultados['cyclicMomentum']['epochs'] += cyclicMomentum_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['cyclicMomentum']['name'] = 'Ciclico Con Momentum'\n",
        "resultados['cyclicMomentum']['lr'] = cyclicMomentum_lr_list\n",
        "resultados['cyclicMomentum']['test_acc'] = resultados['cyclicMomentum']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['cyclicMomentum']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['cyclicMomentum']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['cyclicMomentum']['cost'] = DeleteZerosFromList(DivideList(resultados['cyclicMomentum']['cost'], MAX_ITERATIONS))\n",
        "resultados['cyclicMomentum']['time'] = resultados['cyclicMomentum']['time'] / MAX_ITERATIONS\n",
        "resultados['cyclicMomentum']['epochs'] = resultados['cyclicMomentum']['epochs'] / MAX_ITERATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28gylOT-cNL5"
      },
      "source": [
        "## Tasa de aprendizaje cíclico aleatorio con Momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$base\\_ta: 1x10^{-2}$\n",
        "\n",
        "$max\\_ta: 1x10^{-1}$\n",
        "\n",
        "$n: 3$ epochs\n",
        "\n",
        "$beta: 0.9$"
      ],
      "metadata": {
        "id": "oIvRnoeZhY0v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "0_7uf-LUcQuE"
      },
      "outputs": [],
      "source": [
        "def CyclicGD_Momentum():\n",
        "    import random\n",
        "    modelRandomCyclic_Momentum= nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                                              nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                                              nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelRandomCyclic_Momentum.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    clr_fn = lambda x, y: random.uniform(x, y)\n",
        "\n",
        "    scheduler = CyclicLRGiselt_Denis(optimizer, base_lr=0.01, max_lr=0.1,step_size_up=2, scale_fn=clr_fn,scale_mode='chipichipi', cycle_momentum=False)\n",
        "\n",
        "    start.record()\n",
        "    random_cyclic_Momentum_acc_list, random_cyclic_Momentum_cost_list, random_cyclic_Momentum_lr_list, random_cyclic_epochs= train(modelRandomCyclic_Momentum, optimizer,scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    random_cyclic_Momentum_time = start.elapsed_time(end)\n",
        "\n",
        "    random_cyclic_Momentum_acc = accuracy(modelRandomCyclic_Momentum, x_test_tensor,  y_test_tensor, mb_size) \n",
        "\n",
        "    return random_cyclic_Momentum_acc_list, random_cyclic_Momentum_cost_list, random_cyclic_Momentum_lr_list, random_cyclic_Momentum_time, random_cyclic_Momentum_acc, random_cyclic_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "Zikwca8lcrI-"
      },
      "outputs": [],
      "source": [
        "resultados['random_cyclic_Momentum'] = {}\n",
        "resultados['random_cyclic_Momentum']['val_acc_list'] = [0] * epochs\n",
        "resultados['random_cyclic_Momentum']['test_acc'] = 0\n",
        "resultados['random_cyclic_Momentum']['cost'] = [0] * epochs\n",
        "resultados['random_cyclic_Momentum']['time'] = 0\n",
        "resultados['random_cyclic_Momentum']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    random_cyclic_Momentum_acc_list, random_cyclic_Momentum_cost_list, random_cyclic_Momentum_lr_list, random_cyclic_Momentum_time, random_cyclic_Momentum_acc, random_cyclic_epochs = CyclicGD_Momentum()\n",
        "    a = SumList(resultados['random_cyclic_Momentum']['val_acc_list'], random_cyclic_Momentum_acc_list)\n",
        "    resultados['random_cyclic_Momentum']['val_acc_list'] = SumList(resultados['random_cyclic_Momentum']['val_acc_list'], random_cyclic_Momentum_acc_list)\n",
        "    resultados['random_cyclic_Momentum']['test_acc'] += random_cyclic_Momentum_acc\n",
        "    resultados['random_cyclic_Momentum']['cost'] = SumList(resultados['random_cyclic_Momentum']['cost'], random_cyclic_Momentum_cost_list)\n",
        "    resultados['random_cyclic_Momentum']['time'] += random_cyclic_Momentum_time\n",
        "    resultados['random_cyclic_Momentum']['epochs'] += random_cyclic_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['random_cyclic_Momentum']['name'] = 'Random Ciclico con Momentum'\n",
        "resultados['random_cyclic_Momentum']['lr'] = random_cyclic_Momentum_lr_list\n",
        "resultados['random_cyclic_Momentum']['test_acc'] = resultados['random_cyclic_Momentum']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['random_cyclic_Momentum']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['random_cyclic_Momentum']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['random_cyclic_Momentum']['cost'] = DeleteZerosFromList(DivideList(resultados['random_cyclic_Momentum']['cost'], MAX_ITERATIONS))\n",
        "resultados['random_cyclic_Momentum']['time'] = resultados['random_cyclic_Momentum']['time']/ MAX_ITERATIONS\n",
        "resultados['random_cyclic_Momentum']['epochs'] = resultados['random_cyclic_Momentum']['epochs']/ MAX_ITERATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasa de aprendizaje decreciente con Momentum"
      ],
      "metadata": {
        "id": "YH7jqvDOkCBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\alpha: 1x10^{-2}$\n",
        "\n",
        "$beta: 0.9$"
      ],
      "metadata": {
        "id": "RAZhabjjhfgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Our_DecayMomentum():\n",
        "    modelOurDecay = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                                  nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                                  nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelOurDecay.parameters(), lr=1, momentum=0.9)\n",
        "\n",
        "\n",
        "    clr_fn = lambda x, y: random.uniform(x, y)\n",
        "    scheduler = CyclicLRGiselt_Denis(optimizer, base_lr=0.0001, max_lr=0.1,scale_fn=clr_fn, step_size_up=1,scale_mode='decrecimiento', cycle_momentum=False)\n",
        "\n",
        "    start.record()\n",
        "    our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_epochs = train(modelOurDecay,optimizer, scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    our_decay_time = start.elapsed_time(end)\n",
        "\n",
        "    our_decay_acc = accuracy(modelOurDecay, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_time, our_decay_acc, our_decay_epochs"
      ],
      "metadata": {
        "id": "49T-DodfkFXZ"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultados['our_decay_momentum'] = {}\n",
        "resultados['our_decay_momentum']['val_acc_list'] = [0] * epochs\n",
        "resultados['our_decay_momentum']['test_acc'] = 0\n",
        "resultados['our_decay_momentum']['cost'] = [0] * epochs\n",
        "resultados['our_decay_momentum']['time'] = 0\n",
        "resultados['our_decay_momentum']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    our_decay_momentum_acc_list, our_decay_momentum_cost_list, our_decay_momentum_lr_list, our_decay_momentum_time, our_decay_momentum_acc, our_decay_momentum_epochs = Our_DecayMomentum()\n",
        "    resultados['our_decay_momentum']['val_acc_list'] = SumList(resultados['our_decay_momentum']['val_acc_list'], our_decay_momentum_acc_list)\n",
        "    resultados['our_decay_momentum']['test_acc'] += our_decay_momentum_acc\n",
        "    resultados['our_decay_momentum']['cost'] = SumList(resultados['our_decay_momentum']['cost'], our_decay_momentum_cost_list)\n",
        "    resultados['our_decay_momentum']['time'] += our_decay_momentum_time\n",
        "    resultados['our_decay_momentum']['epochs'] += our_decay_momentum_epochs\n",
        "#Saving results\n",
        "resultados['our_decay_momentum']['name'] = 'Nuestro decreciente con Momentum'\n",
        "resultados['our_decay_momentum']['lr'] = our_decay_momentum_lr_list\n",
        "resultados['our_decay_momentum']['test_acc'] = resultados['our_decay_momentum']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['our_decay_momentum']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['our_decay_momentum']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['our_decay_momentum']['cost'] = DeleteZerosFromList(DivideList(resultados['our_decay_momentum']['cost'], MAX_ITERATIONS))\n",
        "resultados['our_decay_momentum']['time'] = resultados['our_decay_momentum']['time'] / MAX_ITERATIONS\n",
        "resultados['our_decay_momentum']['epochs'] = resultados['our_decay_momentum']['epochs'] / MAX_ITERATIONS"
      ],
      "metadata": {
        "id": "pmom3iiZkJP6"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8_h21aMdgsB"
      },
      "source": [
        "# Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJQ2_Q_q-s3a"
      },
      "source": [
        "Al guardar todos los resultados de cada método en sus respectivos diccionarios, se procede a crear un dataframe con estos resultados. Adicionalmente, se generan nuevas columnas derivadas de datos que ya disponemos y se hacen tratamiento de formato para su análisis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, _ in resultados.items():\n",
        "    if( resultados[key]['val_acc_list'][0] == 0 ):\n",
        "        continue\n",
        "    resultados[key]['val_acc_list'].insert(0,0)"
      ],
      "metadata": {
        "id": "niADA484gEHD"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "7S5hiFtm-9xj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "resultados_df = pd.DataFrame(resultados.copy()).T\n",
        "\n",
        "resultados_df['val_acc'] = resultados_df.apply(lambda row: round(row['val_acc_list'][-1]* 100,2), axis=1)\n",
        "resultados_df['test_acc'] = resultados_df.apply(lambda row: round(row['test_acc']*100,2), axis=1)\n",
        "resultados_df['epochs'] = resultados_df.apply(lambda row: len(row['val_acc_list']), axis=1)\n",
        "resultados_df['val_acc'] = resultados_df.apply(lambda row: '{acc}%'.format(acc = row['val_acc']), axis=1)\n",
        "resultados_df['test_acc'] = resultados_df.apply(lambda row: '{acc}%'.format(acc = row['test_acc']), axis=1)\n",
        "\n",
        "resultados_df = resultados_df.sort_values(by=['test_acc'],ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Qelt3wIs3T"
      },
      "source": [
        "## Velocidad de convergencia basado en epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4Lkzc-79jsV8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "lruyulJ5C4SR",
        "outputId": "86147ed8-4551-4018-a09c-eabea399d8f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-187-76e41c7e5c67>:1: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
            "  resultados_df[['name','val_acc', 'test_acc', 'epochs']].style.hide_index()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f1bf52ede20>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_12080\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_12080_level0_col0\" class=\"col_heading level0 col0\" >name</th>\n",
              "      <th id=\"T_12080_level0_col1\" class=\"col_heading level0 col1\" >val_acc</th>\n",
              "      <th id=\"T_12080_level0_col2\" class=\"col_heading level0 col2\" >test_acc</th>\n",
              "      <th id=\"T_12080_level0_col3\" class=\"col_heading level0 col3\" >epochs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_12080_row0_col0\" class=\"data row0 col0\" >Nuestro decreciente con Momentum</td>\n",
              "      <td id=\"T_12080_row0_col1\" class=\"data row0 col1\" >96.09%</td>\n",
              "      <td id=\"T_12080_row0_col2\" class=\"data row0 col2\" >95.48%</td>\n",
              "      <td id=\"T_12080_row0_col3\" class=\"data row0 col3\" >6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_12080_row1_col0\" class=\"data row1 col0\" >Random Ciclico con Momentum</td>\n",
              "      <td id=\"T_12080_row1_col1\" class=\"data row1 col1\" >95.04%</td>\n",
              "      <td id=\"T_12080_row1_col2\" class=\"data row1 col2\" >94.97%</td>\n",
              "      <td id=\"T_12080_row1_col3\" class=\"data row1 col3\" >7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_12080_row2_col0\" class=\"data row2 col0\" >Ciclico Con Momentum</td>\n",
              "      <td id=\"T_12080_row2_col1\" class=\"data row2 col1\" >95.26%</td>\n",
              "      <td id=\"T_12080_row2_col2\" class=\"data row2 col2\" >94.85%</td>\n",
              "      <td id=\"T_12080_row2_col3\" class=\"data row2 col3\" >7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_12080_row3_col0\" class=\"data row3 col0\" >Random Ciclico</td>\n",
              "      <td id=\"T_12080_row3_col1\" class=\"data row3 col1\" >95.24%</td>\n",
              "      <td id=\"T_12080_row3_col2\" class=\"data row3 col2\" >94.46%</td>\n",
              "      <td id=\"T_12080_row3_col3\" class=\"data row3 col3\" >38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_12080_row4_col0\" class=\"data row4 col0\" >Ciclico</td>\n",
              "      <td id=\"T_12080_row4_col1\" class=\"data row4 col1\" >95.17%</td>\n",
              "      <td id=\"T_12080_row4_col2\" class=\"data row4 col2\" >94.38%</td>\n",
              "      <td id=\"T_12080_row4_col3\" class=\"data row4 col3\" >41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_12080_row5_col0\" class=\"data row5 col0\" >Nuestro decreciente</td>\n",
              "      <td id=\"T_12080_row5_col1\" class=\"data row5 col1\" >95.24%</td>\n",
              "      <td id=\"T_12080_row5_col2\" class=\"data row5 col2\" >94.36%</td>\n",
              "      <td id=\"T_12080_row5_col3\" class=\"data row5 col3\" >26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_12080_row6_col0\" class=\"data row6 col0\" >Momentum</td>\n",
              "      <td id=\"T_12080_row6_col1\" class=\"data row6 col1\" >95.21%</td>\n",
              "      <td id=\"T_12080_row6_col2\" class=\"data row6 col2\" >94.19%</td>\n",
              "      <td id=\"T_12080_row6_col3\" class=\"data row6 col3\" >25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ],
      "source": [
        "resultados_df[['name','val_acc', 'test_acc', 'epochs']].style.hide_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhAah9L2cmoj"
      },
      "source": [
        "## Trazas del desempeño"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "bP3C7xC1vql8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "90524913-f0c5-4557-f5bc-ef550a1694b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nbbox = dict(boxstyle =\"round\", fc =\"1\")\\narrowprops = dict(\\n    arrowstyle = \"->\")\\noffset = 72\\n\\nx_axis = range(100)\\ny_axis = np.linspace(0,5,10)\\n\\nfig, ax = plt.subplots(1,2,figsize=(7, 5))\\n\\n\\n#Precisiones\\nfor k1, _ in resultados.items():\\n    ax[1,1].plot(range(len(resultados[k1][\\'val_acc_list\\'])),\\n        resultados[k1][\\'val_acc_list\\'],\\n        label=\\'{name} = {acc}%\\'.format(name= resultados[k1][\\'name\\'],\\n                                       acc = round(resultados[k1][\\'val_acc_list\\'][-1] * 100 , 2)))\\n\\nplt.title(\\'Traza de la precisión\\', fontsize = 18)\\n\\nax[1,1].set_xlabel(\\'# Epochs\\', fontsize = 18)\\nax[1,1].set_ylabel(\\'Train Accuracy\\', fontsize = 18)\\nax[1,1].spines[\\'top\\'].set_visible(False)\\nax[1,1].spines[\\'right\\'].set_visible(False)\\nax[1,1].xaxis.set_major_locator(MultipleLocator(10))\\nax[1,1].xaxis.set_minor_locator(MultipleLocator(1))\\nax[1,1].legend()\\n\\n#Costos\\nfor k1, _ in resultados.items():\\n    ax[1,2].plot(range(len(resultados[k1][\\'cost\\'])),\\n        resultados[k1][\\'cost\\'],\\n        label=\\'{name} = {acc}%\\'.format(name= resultados[k1][\\'name\\'],\\n                                       acc = round(resultados[k1][\\'cost\\'][-1] * 100 , 2)))\\n\\nplt.title(\\'Traza de los costos\\', fontsize = 18)\\n\\nax[1,2].set_xlabel(\\'# Epochs\\', fontsize = 18)\\nax[1,2].set_ylabel(\\'Costo de pérdida\\', fontsize = 18)\\nax[1,2].spines[\\'top\\'].set_visible(False)\\nax[1,2].spines[\\'right\\'].set_visible(False)\\nax[1,2].xaxis.set_major_locator(MultipleLocator(10))\\nax[1,2].xaxis.set_minor_locator(MultipleLocator(1))\\nax[1,2].legend()\\n\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 188
        }
      ],
      "source": [
        "from matplotlib.ticker import MultipleLocator\n",
        "'''\n",
        "bbox = dict(boxstyle =\"round\", fc =\"1\")\n",
        "arrowprops = dict(\n",
        "    arrowstyle = \"->\")\n",
        "offset = 72\n",
        "\n",
        "x_axis = range(100)\n",
        "y_axis = np.linspace(0,5,10)\n",
        "\n",
        "fig, ax = plt.subplots(1,2,figsize=(7, 5))\n",
        "\n",
        "\n",
        "#Precisiones\n",
        "for k1, _ in resultados.items():\n",
        "    ax[1,1].plot(range(len(resultados[k1]['val_acc_list'])),\n",
        "        resultados[k1]['val_acc_list'],\n",
        "        label='{name} = {acc}%'.format(name= resultados[k1]['name'],\n",
        "                                       acc = round(resultados[k1]['val_acc_list'][-1] * 100 , 2)))\n",
        "\n",
        "plt.title('Traza de la precisión', fontsize = 18)\n",
        "\n",
        "ax[1,1].set_xlabel('# Epochs', fontsize = 18)\n",
        "ax[1,1].set_ylabel('Train Accuracy', fontsize = 18)\n",
        "ax[1,1].spines['top'].set_visible(False)\n",
        "ax[1,1].spines['right'].set_visible(False)\n",
        "ax[1,1].xaxis.set_major_locator(MultipleLocator(10))\n",
        "ax[1,1].xaxis.set_minor_locator(MultipleLocator(1))\n",
        "ax[1,1].legend()\n",
        "\n",
        "#Costos\n",
        "for k1, _ in resultados.items():\n",
        "    ax[1,2].plot(range(len(resultados[k1]['cost'])),\n",
        "        resultados[k1]['cost'],\n",
        "        label='{name} = {acc}%'.format(name= resultados[k1]['name'],\n",
        "                                       acc = round(resultados[k1]['cost'][-1] * 100 , 2)))\n",
        "\n",
        "plt.title('Traza de los costos', fontsize = 18)\n",
        "\n",
        "ax[1,2].set_xlabel('# Epochs', fontsize = 18)\n",
        "ax[1,2].set_ylabel('Costo de pérdida', fontsize = 18)\n",
        "ax[1,2].spines['top'].set_visible(False)\n",
        "ax[1,2].spines['right'].set_visible(False)\n",
        "ax[1,2].xaxis.set_major_locator(MultipleLocator(10))\n",
        "ax[1,2].xaxis.set_minor_locator(MultipleLocator(1))\n",
        "ax[1,2].legend()\n",
        "\n",
        "plt.show()\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "ccfea65839ea7b4cb7611917847ae955f8a5d3f7496a05c88f3bf628abe673bf"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}