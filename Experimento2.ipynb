{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenisPerez/NN_Tests_DG/blob/main/Experimento2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgNAaM97oPwU"
      },
      "source": [
        "Tomar referencia de los archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBCruOrtR_cV",
        "outputId": "feef8ee0-1b83-453c-af34-e26cf49cfedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NN_Tests_DG'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 75 (delta 30), reused 49 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (75/75), 13.71 MiB | 2.07 MiB/s, done.\n",
            "/content/NN_Tests_DG\n",
            "classes.py          get_images.py             LearningRateStudy.ipynb\n",
            "\u001b[0m\u001b[01;34mDataset\u001b[0m/            Iteracion1.ipynb          NN_from_Scratch.ipynb\n",
            "Decay.ipynb         Iteracion1_new.ipynb      NN_Pytorch.ipynb\n",
            "Experimento2.ipynb  Iteracion_2.ipynb         ResNet56_cifar10.ipynb\n",
            "Experimento3.ipynb  Iteracion3_Mejores.ipynb  SGD_Momentum_RMSProp_Adam.ipynb\n"
          ]
        }
      ],
      "source": [
        "!git clone https://ginpg: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@github.com/DenisPerez/NN_Tests_DG\n",
        "%cd NN_Tests_DG\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F4gP7LPQTtg7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from get_images import get_images\n",
        "\n",
        "# PyTorch \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-mULGvWTthB"
      },
      "source": [
        "# Get Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9vdIQd58TthD"
      },
      "outputs": [],
      "source": [
        "MNIST_PATH = './Dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "slQPSqQeTthE"
      },
      "outputs": [],
      "source": [
        "x_train_num, y_train_num, x_test_num, y_test_num = get_images(MNIST_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HFnaRwd-TthE"
      },
      "outputs": [],
      "source": [
        "#Training set\n",
        "x_train = x_train_num[:50000].reshape(50000,-1).astype(np.float32)/255 ##Convert the traint set into a (50000, 28x28) matrix normalized\n",
        "y_train = y_train_num[:50000].reshape(50000,1)\n",
        "\n",
        "##Validation set\n",
        "x_val = x_train_num[50000:].reshape(10000,-1).astype(np.float32)/255\n",
        "y_val = y_train_num[50000:].reshape(10000,1)\n",
        "\n",
        "##Test set\n",
        "x_test = x_test_num.copy().reshape(10000,-1).astype(np.float32)/255\n",
        "y_test = y_test_num.copy().reshape(10000,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZz8hG0OTthF"
      },
      "source": [
        "## Normalize images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_kFtDZs6TthG"
      },
      "outputs": [],
      "source": [
        "def normalise(x_mean, x_std, x_data):\n",
        "    return (x_data - x_mean) / x_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KYhJenIZTthG"
      },
      "outputs": [],
      "source": [
        "x_mean = x_train.mean()\n",
        "x_std = x_train.std()\n",
        "\n",
        "x_train = normalise(x_mean, x_std, x_train)\n",
        "x_val = normalise(x_mean, x_std, x_val)\n",
        "x_test = normalise(x_mean, x_std, x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRx3THErTthH",
        "outputId": "c5e606a8-1147-4056-c45f-f206e0c6557b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8.5686665e-08, 0.9999983)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "x_train.mean(), x_train.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMAv3EVXTthJ"
      },
      "source": [
        "## Show Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHHAe687TthJ",
        "outputId": "12f9e861-8e4a-400f-f73e-e869105358f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_FPmhofTthK",
        "outputId": "6e4311aa-eb9f-45ae-8ff0-4d87d3a564d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w4HfkSTqTthK"
      },
      "outputs": [],
      "source": [
        "def plot_number(image):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "7TQtWTpZTthL",
        "outputId": "9989503c-26a2-45d2-e281-414ac9502c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La imagen muestreada representa un: 9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "rnd_idx = np.random.randint(len(y_test))\n",
        "print(f'La imagen muestreada representa un: {y_test[rnd_idx, 0]}')\n",
        "plot_number(x_test_num[rnd_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVo-PF6cTthM"
      },
      "source": [
        "## Create Mini Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ym639R_PTthM"
      },
      "outputs": [],
      "source": [
        "def create_minibatches(x, y, mb_size, shuffle = True):\n",
        "    '''\n",
        "    x  #muestras, input_layer\n",
        "    y #muestras, 1\n",
        "    '''\n",
        "    assert x.shape[0] == y.shape[0], 'Error en cantidad de muestras'\n",
        "    total_data = x.shape[0]\n",
        "    if shuffle: \n",
        "        idxs = np.arange(total_data, dtype=float)\n",
        "        np.random.shuffle(idxs)\n",
        "        x = x[idxs]\n",
        "        y = y[idxs]  \n",
        "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5Ypqe6nTthN"
      },
      "source": [
        "# Pytorch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6quE_eQ9TthO"
      },
      "outputs": [],
      "source": [
        "x_train_tensor = torch.Tensor(x_train.copy())\n",
        "y_train_tensor = torch.Tensor(y_train.copy())\n",
        "\n",
        "x_val_tensor = torch.Tensor(x_val.copy())\n",
        "y_val_tensor = torch.Tensor(y_val.copy())\n",
        "\n",
        "x_test_tensor = torch.Tensor(x_test.copy())\n",
        "y_test_tensor = torch.Tensor(y_test.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqPgXB1DTthP"
      },
      "source": [
        "## Use GPU when available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1oc0PTWTthQ",
        "outputId": "2d862465-b692-4611-e265-c8a35edabf3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZebY-iBHTthQ",
        "outputId": "2c0af8a9-e1f0-49c7-da68-971874a67ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estamos usando: cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(f'Estamos usando: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L66abJ9WTthR"
      },
      "source": [
        "# Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Zt6iiZyuTthR"
      },
      "outputs": [],
      "source": [
        "def accuracy(model: nn.Sequential, x: torch.tensor, y: torch.tensor, mb_size: int):\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    model.eval()\n",
        "    model = model.to(device=device)\n",
        "    with torch.no_grad():\n",
        "        for (xi, yi) in create_minibatches(x, y, mb_size):\n",
        "            xi = xi.to(device=device, dtype = torch.float32)\n",
        "            yi = yi.to(device=device, dtype = torch.long)\n",
        "            scores = model(xi) # mb_size, 10\n",
        "            _, pred = scores.max(dim=1) #pred shape (mb_size )\n",
        "            num_correct += (pred == yi.squeeze()).sum() # pred shape (mb_size), yi shape (mb_size, 1)\n",
        "            num_total += pred.size(0)\n",
        "\n",
        "            return float(num_correct)/num_total  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR4DuKf3TthR"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HtRDD6Nu0507"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, scheduler: None, mb_size):\n",
        "    model = model.to(device=device)\n",
        "    epoch_acc = 0.0\n",
        "    i = 0\n",
        "    #plot lists\n",
        "    acc_list = [0.0]\n",
        "    cost_list = [0.0]\n",
        "    lr_list = [0.0]\n",
        "    while (epoch_acc < 0.95 and i < 100):\n",
        "        for (xi, yi) in create_minibatches(x_train_tensor, y_train_tensor, mb_size):\n",
        "            model.train()\n",
        "            xi = xi.to(device=device, dtype=torch.float32)\n",
        "            yi = yi.to(device=device, dtype=torch.long)\n",
        "            scores = model(xi)\n",
        "            # cost function\n",
        "            cost = F.cross_entropy(input= scores, target=yi.squeeze())\n",
        "            optimizer.zero_grad()\n",
        "            cost.backward()\n",
        "            for name, param in model.named_parameters():\n",
        "              ik = str(name)+'_'+str(i)\n",
        "              prev_ik = str(name)+'_'+str(i-1)\n",
        "            optimizer.step()\n",
        "        if (scheduler != None):\n",
        "          scheduler.step()\n",
        "          lr = scheduler.get_last_lr()\n",
        "          lr_list.append(lr[0])\n",
        "        else: \n",
        "          lr_list.append(optimizer.param_groups[0]['lr'])\n",
        "        i+=1\n",
        "        epoch_acc = accuracy(model, x_val_tensor, y_val_tensor, mb_size)\n",
        "        epoch_cost = cost.item()\n",
        "\n",
        "        #append\n",
        "        acc_list.append(epoch_acc)\n",
        "        cost_list.append(epoch_cost)\n",
        "        # print(f'Epoch: {len(acc_list)}, learning_rate:{lr},costo: {epoch_cost}, accuracy: {epoch_acc}')\n",
        "    return acc_list, cost_list, lr_list, i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6xxVksqR95C"
      },
      "source": [
        "## List Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5aJV7jOTR95D"
      },
      "outputs": [],
      "source": [
        "def SumList(first: list, second: list) -> list:\n",
        "    return [x + y for x, y in zip(first[::-1], second[::-1])][::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "I51hZdnHR95D"
      },
      "outputs": [],
      "source": [
        "def DivideList(dic_list: list, number: int) -> list:\n",
        "    return [x / number for x in dic_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "y6ro6TQRR95E"
      },
      "outputs": [],
      "source": [
        "def DeleteZerosFromList(dic_list: list) -> list:\n",
        "    return list(filter(lambda num: num != 0, dic_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "015SGTIaTthS"
      },
      "source": [
        "# Experimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ2qa3c-eS1L"
      },
      "source": [
        "Global var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WBDuey1VQTm4"
      },
      "outputs": [],
      "source": [
        "MAX_ITERATIONS = 5\n",
        "layer1 = 1000 \n",
        "layer2 = 1000\n",
        "lr = 1e-2\n",
        "epochs = 100\n",
        "mb_size = 4096\n",
        "input_layer = 784\n",
        "first_i = 0\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "resultados = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mAWn11Vmc6-"
      },
      "source": [
        "## Cíclico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_DdXVK_ZR95I"
      },
      "outputs": [],
      "source": [
        "def Cyclic():\n",
        "    modelCyclic = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelCyclic.parameters(), lr=lr)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1,step_size_up=3,cycle_momentum=False )\n",
        "\n",
        "    start.record()\n",
        "    cyclic_acc_list, cyclic_cost_list, cyclic_lr_list, cyclic_epochs= train(modelCyclic, optimizer,scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    cyclic_time = start.elapsed_time(end)\n",
        "\n",
        "    cyclic_acc = accuracy(modelCyclic, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return cyclic_acc_list, cyclic_cost_list, cyclic_lr_list, cyclic_time, cyclic_acc, cyclic_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qWp659FGmhDK"
      },
      "outputs": [],
      "source": [
        "resultados['cyclic'] = {}\n",
        "resultados['cyclic']['val_acc_list'] = [0]* epochs\n",
        "resultados['cyclic']['test_acc'] = 0\n",
        "resultados['cyclic']['cost'] = [0] * epochs\n",
        "resultados['cyclic']['time'] = 0\n",
        "resultados['cyclic']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    cyclic_acc_list, cyclic_cost_list, cyclic_lr_list, cyclic_time, cyclic_acc, cyclic_epochs = Cyclic()\n",
        "    resultados['cyclic']['val_acc_list'] = SumList(resultados['cyclic']['val_acc_list'], cyclic_acc_list)\n",
        "    resultados['cyclic']['test_acc'] += cyclic_acc\n",
        "    resultados['cyclic']['cost'] = SumList(resultados['cyclic']['cost'], cyclic_cost_list)\n",
        "    resultados['cyclic']['time'] += cyclic_time\n",
        "    resultados['cyclic']['time'] += cyclic_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['cyclic']['name'] = 'Ciclico'\n",
        "resultados['cyclic']['lr'] = cyclic_lr_list\n",
        "resultados['cyclic']['test_acc'] = resultados['cyclic']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['cyclic']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['cyclic']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['cyclic']['cost'] = DeleteZerosFromList(DivideList(resultados['cyclic']['cost'], MAX_ITERATIONS))\n",
        "resultados['cyclic']['time'] = resultados['cyclic']\n",
        "resultados['cyclic']['epochs'] = resultados['cyclic']['epochs']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj0arCxmZdxw"
      },
      "source": [
        "## Cyclic Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9vrI2SE_3zaR"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "\n",
        "import types\n",
        "import math\n",
        "from torch._six import inf\n",
        "from functools import wraps\n",
        "import warnings\n",
        "import weakref\n",
        "import random\n",
        "\n",
        "class _LRSchedulerGiselt_Denis(object):\n",
        "\n",
        "    def __init__(self, optimizer, last_epoch=-1, verbose=False):\n",
        "\n",
        "        # Attach optimizer\n",
        "        if not isinstance(optimizer, Optimizer):\n",
        "            raise TypeError('{} is not an Optimizer'.format(\n",
        "                type(optimizer).__name__))\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initialize epoch and base learning rates\n",
        "        if last_epoch == -1:\n",
        "            for group in optimizer.param_groups:\n",
        "                group.setdefault('initial_lr', group['lr'])\n",
        "        else:\n",
        "            for i, group in enumerate(optimizer.param_groups):\n",
        "                if 'initial_lr' not in group:\n",
        "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
        "                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
        "        self.base_lrs = [group['initial_lr'] for group in optimizer.param_groups]\n",
        "        self.last_epoch = last_epoch\n",
        "\n",
        "        # Following https://github.com/pytorch/pytorch/issues/20124\n",
        "        # We would like to ensure that `lr_scheduler.step()` is called after\n",
        "        # `optimizer.step()`\n",
        "        def with_counter(method):\n",
        "            if getattr(method, '_with_counter', False):\n",
        "                # `optimizer.step()` has already been replaced, return.\n",
        "                return method\n",
        "\n",
        "            # Keep a weak reference to the optimizer instance to prevent\n",
        "            # cyclic references.\n",
        "            instance_ref = weakref.ref(method.__self__)\n",
        "            # Get the unbound method for the same purpose.\n",
        "            func = method.__func__\n",
        "            cls = instance_ref().__class__\n",
        "            del method\n",
        "\n",
        "            @wraps(func)\n",
        "            def wrapper(*args, **kwargs):\n",
        "                instance = instance_ref()\n",
        "                instance._step_count += 1\n",
        "                wrapped = func.__get__(instance, cls)\n",
        "                return wrapped(*args, **kwargs)\n",
        "\n",
        "            # Note that the returned function here is no longer a bound method,\n",
        "            # so attributes like `__func__` and `__self__` no longer exist.\n",
        "            wrapper._with_counter = True\n",
        "            return wrapper\n",
        "\n",
        "        self.optimizer.step = with_counter(self.optimizer.step)\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self._initial_step()\n",
        "\n",
        "    def _initial_step(self):\n",
        "        \"\"\"Initialize step counts and performs a step\"\"\"\n",
        "        self.optimizer._step_count = 0\n",
        "        self._step_count = 0\n",
        "        self.step()\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
        "\n",
        "        It contains an entry for every variable in self.__dict__ which\n",
        "        is not the optimizer.\n",
        "        \"\"\"\n",
        "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"\"\"Loads the schedulers state.\n",
        "\n",
        "        Args:\n",
        "            state_dict (dict): scheduler state. Should be an object returned\n",
        "                from a call to :meth:`state_dict`.\n",
        "        \"\"\"\n",
        "        self.__dict__.update(state_dict)\n",
        "\n",
        "    def get_last_lr(self):\n",
        "        \"\"\" Return last computed learning rate by current scheduler.\n",
        "        \"\"\"\n",
        "        return self._last_lr\n",
        "\n",
        "    def get_lr(self):\n",
        "        # Compute learning rate using chainable form of the scheduler\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def print_lr(self, is_verbose, group, lr, epoch=None):\n",
        "        \"\"\"Display the current learning rate.\n",
        "        \"\"\"\n",
        "        if is_verbose:\n",
        "            if epoch is None:\n",
        "                print('Adjusting learning rate'\n",
        "                      ' of group {} to {:.4e}.'.format(group, lr))\n",
        "            else:\n",
        "                epoch_str = (\"%.2f\" if isinstance(epoch, float) else\n",
        "                             \"%.5d\") % epoch\n",
        "                print('Epoch {}: adjusting learning rate'\n",
        "                      ' of group {} to {:.4e}.'.format(epoch_str, group, lr))\n",
        "\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        # Raise a warning if old pattern is detected\n",
        "        # https://github.com/pytorch/pytorch/issues/20124\n",
        "        if self._step_count == 1:\n",
        "            if not hasattr(self.optimizer.step, \"_with_counter\"):\n",
        "                warnings.warn(\"Seems like `optimizer.step()` has been overridden after learning rate scheduler \"\n",
        "                              \"initialization. Please, make sure to call `optimizer.step()` before \"\n",
        "                              \"`lr_scheduler.step()`. See more details at \"\n",
        "                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
        "\n",
        "            # Just check if there were two first lr_scheduler.step() calls before optimizer.step()\n",
        "            elif self.optimizer._step_count < 1:\n",
        "                warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
        "                              \"In PyTorch 1.1.0 and later, you should call them in the opposite order: \"\n",
        "                              \"`optimizer.step()` before `lr_scheduler.step()`.  Failure to do this \"\n",
        "                              \"will result in PyTorch skipping the first value of the learning rate schedule. \"\n",
        "                              \"See more details at \"\n",
        "                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
        "        self._step_count += 1\n",
        "\n",
        "        class _enable_get_lr_call:\n",
        "\n",
        "            def __init__(self, o):\n",
        "                self.o = o\n",
        "\n",
        "            def __enter__(self):\n",
        "                self.o._get_lr_called_within_step = True\n",
        "                return self\n",
        "\n",
        "            def __exit__(self, type, value, traceback):\n",
        "                self.o._get_lr_called_within_step = False\n",
        "\n",
        "        with _enable_get_lr_call(self):\n",
        "            if epoch is None:\n",
        "                self.last_epoch += 1\n",
        "                values = self.get_lr()\n",
        "            else:\n",
        "                warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
        "                self.last_epoch = epoch\n",
        "                if hasattr(self, \"_get_closed_form_lr\"):\n",
        "                    values = self._get_closed_form_lr()\n",
        "                else:\n",
        "                    values = self.get_lr()\n",
        "\n",
        "        for i, data in enumerate(zip(self.optimizer.param_groups, values)):\n",
        "            param_group, lr = data\n",
        "            param_group['lr'] = lr\n",
        "            self.print_lr(self.verbose, i, lr, epoch)\n",
        "\n",
        "        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n",
        "\n",
        "class CyclicLRGiselt_Denis(_LRSchedulerGiselt_Denis):\n",
        "    r\"\"\"Sets the learning rate of each parameter group according to\n",
        "    cyclical learning rate policy (CLR). The policy cycles the learning\n",
        "    rate between two boundaries with a constant frequency, as detailed in\n",
        "    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n",
        "    The distance between the two boundaries can be scaled on a per-iteration\n",
        "    or per-cycle basis.\n",
        "\n",
        "    Cyclical learning rate policy changes the learning rate after every batch.\n",
        "    `step` should be called after a batch has been used for training.\n",
        "\n",
        "    This class has three built-in policies, as put forth in the paper:\n",
        "\n",
        "    * \"triangular\": A basic triangular cycle without amplitude scaling.\n",
        "    * \"triangular2\": A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    * \"exp_range\": A cycle that scales initial amplitude by :math:`\\text{gamma}^{\\text{cycle iterations}}`\n",
        "      at each cycle iteration.\n",
        "\n",
        "    This implementation was adapted from the github repo: `bckenstler/CLR`_\n",
        "\n",
        "    Args:\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        base_lr (float or list): Initial learning rate which is the\n",
        "            lower boundary in the cycle for each parameter group.\n",
        "        max_lr (float or list): Upper learning rate boundaries in the cycle\n",
        "            for each parameter group. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore\n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size_up (int): Number of training iterations in the\n",
        "            increasing half of a cycle. Default: 2000\n",
        "        step_size_down (int): Number of training iterations in the\n",
        "            decreasing half of a cycle. If step_size_down is None,\n",
        "            it is set to step_size_up. Default: None\n",
        "        mode (str): One of {triangular, triangular2, exp_range}.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "            Default: 'triangular'\n",
        "        gamma (float): Constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "            Default: 1.0\n",
        "        scale_fn (function): Custom scaling policy defined by a single\n",
        "            argument lambda function, where\n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            If specified, then 'mode' is ignored.\n",
        "            Default: None\n",
        "        scale_mode (str): {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on\n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle).\n",
        "            Default: 'cycle'\n",
        "        cycle_momentum (bool): If ``True``, momentum is cycled inversely\n",
        "            to learning rate between 'base_momentum' and 'max_momentum'.\n",
        "            Default: True\n",
        "        base_momentum (float or list): Lower momentum boundaries in the cycle\n",
        "            for each parameter group. Note that momentum is cycled inversely\n",
        "            to learning rate; at the peak of a cycle, momentum is\n",
        "            'base_momentum' and learning rate is 'max_lr'.\n",
        "            Default: 0.8\n",
        "        max_momentum (float or list): Upper momentum boundaries in the cycle\n",
        "            for each parameter group. Functionally,\n",
        "            it defines the cycle amplitude (max_momentum - base_momentum).\n",
        "            The momentum at any cycle is the difference of max_momentum\n",
        "            and some scaling of the amplitude; therefore\n",
        "            base_momentum may not actually be reached depending on\n",
        "            scaling function. Note that momentum is cycled inversely\n",
        "            to learning rate; at the start of a cycle, momentum is 'max_momentum'\n",
        "            and learning rate is 'base_lr'\n",
        "            Default: 0.9\n",
        "        last_epoch (int): The index of the last batch. This parameter is used when\n",
        "            resuming a training job. Since `step()` should be invoked after each\n",
        "            batch instead of after each epoch, this number represents the total\n",
        "            number of *batches* computed, not the total number of epochs computed.\n",
        "            When last_epoch=-1, the schedule is started from the beginning.\n",
        "            Default: -1\n",
        "        verbose (bool): If ``True``, prints a message to stdout for\n",
        "            each update. Default: ``False``.\n",
        "\n",
        "    Example:\n",
        "        >>> # xdoctest: +SKIP\n",
        "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "        >>> scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
        "        >>> data_loader = torch.utils.data.DataLoader(...)\n",
        "        >>> for epoch in range(10):\n",
        "        >>>     for batch in data_loader:\n",
        "        >>>         train_batch(...)\n",
        "        >>>         scheduler.step()\n",
        "\n",
        "\n",
        "    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
        "    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 optimizer,\n",
        "                 base_lr,\n",
        "                 max_lr,\n",
        "                 step_size_up=2000,\n",
        "                 step_size_down=None,\n",
        "                 mode='triangular',\n",
        "                 gamma=1.,\n",
        "                 scale_fn=None,\n",
        "                 scale_mode='cycle',\n",
        "                 cycle_momentum=True,\n",
        "                 base_momentum=0.8,\n",
        "                 max_momentum=0.9,\n",
        "                 last_epoch=-1,\n",
        "                 verbose=False):\n",
        "\n",
        "        # Attach optimizer\n",
        "        if not isinstance(optimizer, Optimizer):\n",
        "            raise TypeError('{} is not an Optimizer'.format(\n",
        "                type(optimizer).__name__))\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "        self.direction_up = True\n",
        "        self.half_cycle_steps = 0\n",
        "\n",
        "        base_lrs = self._format_param('base_lr', optimizer, base_lr)\n",
        "        if last_epoch == -1:\n",
        "            for lr, group in zip(base_lrs, optimizer.param_groups):\n",
        "                group['lr'] = lr\n",
        "\n",
        "        self.max_lrs = self._format_param('max_lr', optimizer, max_lr)\n",
        "\n",
        "        self.step_size_up = float(step_size_up)\n",
        "        self.step_size_down = float(step_size_down) if step_size_down is not None else step_size_up\n",
        "        self.total_size = self.step_size_up + self.step_size_down\n",
        "        self.step_ratio = step_size_up / self.total_size\n",
        "\n",
        "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
        "                and scale_fn is None:\n",
        "            raise ValueError('mode is invalid and scale_fn is None')\n",
        "\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "\n",
        "        if scale_fn is None:\n",
        "            self._scale_fn_custom = None\n",
        "            if self.mode == 'triangular':\n",
        "                self._scale_fn_ref = weakref.WeakMethod(self._triangular_scale_fn)\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self._scale_fn_ref = weakref.WeakMethod(self._triangular2_scale_fn)\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self._scale_fn_ref = weakref.WeakMethod(self._exp_range_scale_fn)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self._scale_fn_custom = scale_fn\n",
        "            self._scale_fn_ref = None\n",
        "            self.scale_mode = scale_mode\n",
        "\n",
        "        self.cycle_momentum = cycle_momentum\n",
        "        if cycle_momentum:\n",
        "            if 'momentum' not in optimizer.defaults:\n",
        "                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n",
        "\n",
        "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
        "            if last_epoch == -1:\n",
        "                for momentum, group in zip(base_momentums, optimizer.param_groups):\n",
        "                    group['momentum'] = momentum\n",
        "            self.base_momentums = [group['momentum'] for group in optimizer.param_groups]\n",
        "            self.max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
        "\n",
        "        super(CyclicLRGiselt_Denis, self).__init__(optimizer, last_epoch, verbose)\n",
        "        self.base_lrs = base_lrs\n",
        "\n",
        "    def _format_param(self, name, optimizer, param):\n",
        "        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n",
        "        if isinstance(param, (list, tuple)):\n",
        "            if len(param) != len(optimizer.param_groups):\n",
        "                raise ValueError(\"expected {} values for {}, got {}\".format(\n",
        "                    len(optimizer.param_groups), name, len(param)))\n",
        "            return param\n",
        "        else:\n",
        "            return [param] * len(optimizer.param_groups)\n",
        "\n",
        "    def scale_fn(self, x):\n",
        "        if self._scale_fn_custom is not None:\n",
        "            # print(self.half_cycle_steps, x)\n",
        "            return self._scale_fn_custom(x)\n",
        "\n",
        "        else:\n",
        "            return self._scale_fn_ref()(x)\n",
        "\n",
        "    def scale_fn_rand(self, x, y):\n",
        "        return random.uniform(x, y)\n",
        "\n",
        "    def _triangular_scale_fn(self, x):\n",
        "        return 1.\n",
        "\n",
        "    def _triangular2_scale_fn(self, x):\n",
        "        return 1 / (2. ** (x - 1))\n",
        "\n",
        "    def _exp_range_scale_fn(self, x):\n",
        "        return self.gamma**(x)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"Calculates the learning rate at batch index. This function treats\n",
        "        `self.last_epoch` as the last batch index.\n",
        "\n",
        "        If `self.cycle_momentum` is ``True``, this function has a side effect of\n",
        "        updating the optimizer's momentum.\n",
        "        \"\"\"\n",
        "        \n",
        "\n",
        "        if not self._get_lr_called_within_step:\n",
        "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
        "                          \"please use `get_last_lr()`.\", UserWarning)\n",
        "\n",
        "        cycle = math.floor(1 + self.last_epoch / self.total_size)\n",
        "        x = 1. + self.last_epoch / self.total_size - cycle\n",
        "        if x <= self.step_ratio:\n",
        "            scale_factor = x / self.step_ratio\n",
        "        else:\n",
        "            scale_factor = (x - 1) / (self.step_ratio - 1)\n",
        "\n",
        "        lrs = []\n",
        "        for base_lr, max_lr in zip(self.base_lrs, self.max_lrs):\n",
        "            base_height = (max_lr - base_lr) * scale_factor\n",
        "            if self.scale_mode == 'cycle':\n",
        "\n",
        "                # print(\"cycle\")\n",
        "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
        "            elif self.scale_mode == 'iterations':\n",
        "\n",
        "                # print(\"iterations\")\n",
        "                lr = base_lr + base_height * self.scale_fn(self.last_epoch)\n",
        "\n",
        "            elif self.scale_mode == 'decrecimiento':\n",
        "                if (self.last_epoch == 0):\n",
        "                  lr = max_lr\n",
        "                else: \n",
        "                  lr = self.get_last_lr()[0] - base_lr\n",
        "                  if (lr<0):\n",
        "                    lr = base_lr\n",
        "\n",
        "            elif self.scale_mode == 'chipichipi':                               #<--------------------------\n",
        "                \n",
        "                '''\n",
        "                for x in optimizer.param_groups:\n",
        "                  for each in x['params']:\n",
        "                    print(each)\n",
        "                \n",
        "                for name, param in modelBB.named_parameters():\n",
        "                  print(param.grad.shape)\n",
        "                '''\n",
        "\n",
        "                # print(\"chipichipi\")\n",
        "                if (self.last_epoch == 0):\n",
        "                    last_lr = base_lr\n",
        "                else:\n",
        "                    last_lr = self.get_last_lr()[0]\n",
        "                    # print(f\"Esto es el ultimo lr {last_lr}\")\n",
        "\n",
        "                if (self.half_cycle_steps == self.step_size_up\n",
        "                    and self.direction_up == True):\n",
        "                    #print('Cambio de direccion: bajando')\n",
        "                    lr = random.uniform(last_lr, max_lr)\n",
        "                    self.direction_up = False\n",
        "                    self.half_cycle_steps = 1\n",
        "\n",
        "                elif (self.half_cycle_steps == self.step_size_down\n",
        "                    and self.direction_up == False):\n",
        "                    #print('Cambio de direccion: subiendo')\n",
        "                    lr = random.uniform(base_lr, last_lr)\n",
        "                    self.direction_up = True\n",
        "                    self.half_cycle_steps = 1\n",
        "\n",
        "                elif (self.direction_up == True):\n",
        "                    lr = random.uniform(last_lr, max_lr)\n",
        "                    self.half_cycle_steps += 1\n",
        "\n",
        "                elif(self.direction_up == False):\n",
        "                    lr = random.uniform(base_lr, last_lr)\n",
        "                    self.half_cycle_steps += 1\n",
        "            #print('base_lr: '+str(base_lr))\n",
        "            lrs.append(lr)\n",
        "\n",
        "        if self.cycle_momentum:\n",
        "            #print('wowo')\n",
        "            momentums = []\n",
        "            for base_momentum, max_momentum in zip(self.base_momentums, self.max_momentums):\n",
        "                base_height = (max_momentum - base_momentum) * scale_factor\n",
        "                if self.scale_mode == 'cycle':\n",
        "                    momentum = max_momentum - base_height * self.scale_fn(cycle)\n",
        "                else:\n",
        "                    momentum = max_momentum - base_height * self.scale_fn(self.last_epoch)\n",
        "                #print('momentum: '+str(max_momentum)+' '+str(base_momentum))\n",
        "                momentums.append(momentum)\n",
        "            for param_group, momentum in zip(self.optimizer.param_groups, momentums):\n",
        "                param_group['momentum'] = momentum\n",
        "            \n",
        "        return lrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f8ZZZ1Hcg2R"
      },
      "source": [
        "## Cíclico aleatoria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jywqBaWVR95N"
      },
      "outputs": [],
      "source": [
        "def CyclicGD():\n",
        "    import random\n",
        "    modelRandomCyclic = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                        nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                        nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelRandomCyclic.parameters(), lr=lr)\n",
        "\n",
        "    clr_fn = lambda x, y: random.uniform(x, y)\n",
        "\n",
        "    scheduler = CyclicLRGiselt_Denis(optimizer, base_lr=0.01, max_lr=0.1,step_size_up=3, scale_fn=clr_fn,scale_mode='chipichipi', cycle_momentum=False)\n",
        "\n",
        "    start.record()\n",
        "    random_cyclic_acc_list, random_cyclic_cost_list, random_cyclic_lr_list, random_cyclic_epochs= train(modelRandomCyclic, optimizer,scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    random_cyclic_time = start.elapsed_time(end)\n",
        "\n",
        "    random_cyclic_acc = accuracy(modelRandomCyclic, x_test_tensor,  y_test_tensor, mb_size) \n",
        "\n",
        "    return random_cyclic_acc_list, random_cyclic_cost_list, random_cyclic_lr_list, random_cyclic_time, random_cyclic_acc, random_cyclic_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "acBgxgGkZdL6"
      },
      "outputs": [],
      "source": [
        "resultados['random_cyclic'] = {}\n",
        "resultados['random_cyclic']['val_acc_list'] = [0] * epochs\n",
        "resultados['random_cyclic']['test_acc'] = 0\n",
        "resultados['random_cyclic']['cost'] = [0] * epochs\n",
        "resultados['random_cyclic']['time'] = 0\n",
        "resultados['random_cyclic']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    random_cyclic_acc_list, random_cyclic_cost_list, random_cyclic_lr_list, random_cyclic_time, random_cyclic_acc, random_cyclic_epochs = CyclicGD()\n",
        "    a = SumList(resultados['random_cyclic']['val_acc_list'], random_cyclic_acc_list)\n",
        "    resultados['random_cyclic']['val_acc_list'] = SumList(resultados['random_cyclic']['val_acc_list'], random_cyclic_acc_list)\n",
        "    resultados['random_cyclic']['test_acc'] += random_cyclic_acc\n",
        "    resultados['random_cyclic']['cost'] = SumList(resultados['random_cyclic']['cost'], random_cyclic_cost_list)\n",
        "    resultados['random_cyclic']['time'] += random_cyclic_time\n",
        "    resultados['random_cyclic']['epochs'] += random_cyclic_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['random_cyclic']['name'] = 'Random Ciclico'\n",
        "resultados['random_cyclic']['lr'] = random_cyclic_lr_list\n",
        "resultados['random_cyclic']['test_acc'] = resultados['random_cyclic']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['random_cyclic']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['random_cyclic']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['random_cyclic']['cost'] = DeleteZerosFromList(DivideList(resultados['random_cyclic']['cost'], MAX_ITERATIONS))\n",
        "resultados['random_cyclic']['time'] = resultados['random_cyclic']['time']\n",
        "resultados['random_cyclic']['epochs'] = resultados['random_cyclic']['epochs']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzehV-6emhDY"
      },
      "source": [
        "## Momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Bt6bkSegR95R"
      },
      "outputs": [],
      "source": [
        "def SGDM():\n",
        "    modelSGDM = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimiserSGDM = torch.optim.SGD(modelSGDM.parameters(), lr=lr, momentum=0.9)\n",
        "    start.record()\n",
        "    SGDM_acc_list, SGDM_cost_list,SGDM_lr_list, SGDM_epochs = train(modelSGDM, optimiserSGDM,None, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    SGDM_time = start.elapsed_time(end)\n",
        "\n",
        "    SGDM_acc = accuracy(modelSGDM, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return SGDM_acc_list, SGDM_cost_list, SGDM_lr_list, SGDM_time, SGDM_acc, SGDM_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "U4bdgJL-mkx3"
      },
      "outputs": [],
      "source": [
        "resultados['SGDM'] = {}\n",
        "resultados['SGDM']['val_acc_list'] = [0] * epochs\n",
        "resultados['SGDM']['test_acc'] = 0\n",
        "resultados['SGDM']['cost'] = [0] * epochs\n",
        "resultados['SGDM']['time'] = 0\n",
        "resultados['SGDM']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    SGDM_acc_list, SGDM_cost_list, SGDM_lr_list, SGDM_time, SGDM_acc, SGDM_epochs = SGDM()\n",
        "    resultados['SGDM']['val_acc_list'] = SumList(resultados['SGDM']['val_acc_list'], SGDM_acc_list)\n",
        "    resultados['SGDM']['test_acc'] += SGDM_acc\n",
        "    resultados['SGDM']['cost'] = SumList(resultados['SGDM']['cost'], SGDM_cost_list)\n",
        "    resultados['SGDM']['time'] += SGDM_time\n",
        "    resultados['SGDM']['epochs'] += SGDM_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['SGDM']['name'] = 'Momentum'\n",
        "resultados['SGDM']['lr'] = SGDM_lr_list\n",
        "resultados['SGDM']['test_acc'] = resultados['SGDM']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['SGDM']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['SGDM']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['SGDM']['cost'] = DeleteZerosFromList(DivideList(resultados['SGDM']['cost'], MAX_ITERATIONS))\n",
        "resultados['SGDM']['time'] = resultados['SGDM']['time']\n",
        "resultados['SGDM']['epochs'] = resultados['SGDM']['epochs']\n",
        "\n",
        "# #Saving results\n",
        "# resultados['SGDM'] = {}\n",
        "# resultados['SGDM']['name'] = 'Momentum'\n",
        "# resultados['SGDM']['lr'] = [0]\n",
        "# resultados['SGDM']['val_acc_list'] = SGDM_acc_list\n",
        "# resultados['SGDM']['test_acc'] = SGDM_acc\n",
        "# resultados['SGDM']['cost'] = SGDM_cost_list\n",
        "# resultados['SGDM']['time'] = SGDM_time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nuestro decreciente"
      ],
      "metadata": {
        "id": "VWVaAGYywZbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Our_Decay():\n",
        "    modelOurDecay = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelOurDecay.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "    clr_fn = lambda x, y: random.uniform(x, y)\n",
        "    scheduler = CyclicLRGiselt_Denis(optimizer, base_lr=0.0001, max_lr=0.1,scale_fn=clr_fn, step_size_up=1,scale_mode='decrecimiento', cycle_momentum=False)\n",
        "\n",
        "    start.record()\n",
        "    our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_epochs = train(modelOurDecay,optimizer, scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    our_decay_time = start.elapsed_time(end)\n",
        "\n",
        "    our_decay_acc = accuracy(modelOurDecay, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_time, our_decay_acc, our_decay_epochs"
      ],
      "metadata": {
        "id": "874_mQHJwb3X"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultados['our_decay'] = {}\n",
        "resultados['our_decay']['val_acc_list'] = [0] * epochs\n",
        "resultados['our_decay']['test_acc'] = 0\n",
        "resultados['our_decay']['cost'] = [0] * epochs\n",
        "resultados['our_decay']['time'] = 0\n",
        "resultados['our_decay']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_time, our_decay_acc, our_decay_epochs = Our_Decay()\n",
        "    resultados['our_decay']['val_acc_list'] = SumList(resultados['our_decay']['val_acc_list'], our_decay_acc_list)\n",
        "    resultados['our_decay']['test_acc'] += our_decay_acc\n",
        "    resultados['our_decay']['cost'] = SumList(resultados['our_decay']['cost'], our_decay_cost_list)\n",
        "    resultados['our_decay']['time'] += our_decay_time\n",
        "    resultados['our_decay']['epochs'] += our_decay_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['our_decay']['name'] = 'Nuestro decreciente'\n",
        "resultados['our_decay']['lr'] = our_decay_lr_list\n",
        "resultados['our_decay']['test_acc'] = resultados['our_decay']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['our_decay']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['our_decay']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['our_decay']['cost'] = DeleteZerosFromList(DivideList(resultados['our_decay']['cost'], MAX_ITERATIONS))\n",
        "resultados['our_decay']['time'] = resultados['our_decay']['time'] \n",
        "resultados['our_decay']['epochs'] = resultados['our_decay']['epochs'] / MAX_ITERATIONS"
      ],
      "metadata": {
        "id": "9f3dwRmBwe-2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy_ipT4cV40p"
      },
      "source": [
        "## Cíclico con Momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "z_1Gg-3eWMZr"
      },
      "outputs": [],
      "source": [
        "def CyclicMomentum():\n",
        "    modelCyclicMomentum = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelCyclicMomentum.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1,step_size_up=3,cycle_momentum=False )\n",
        "\n",
        "    start.record()\n",
        "    cyclicMomentum_acc_list, cyclicMomentum_cost_list, cyclicMomentum_lr_list, cyclicMomentum_epochs = train(modelCyclicMomentum, optimizer,scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    cyclicMomentum_time = start.elapsed_time(end)\n",
        "\n",
        "    cyclicMomentum_acc = accuracy(modelCyclicMomentum, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return cyclicMomentum_acc_list, cyclicMomentum_cost_list, cyclicMomentum_lr_list, cyclicMomentum_time, cyclicMomentum_acc, cyclicMomentum_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GPA1FfOOWOtV"
      },
      "outputs": [],
      "source": [
        "resultados['cyclicMomentum'] = {}\n",
        "resultados['cyclicMomentum']['val_acc_list'] = [0]* epochs\n",
        "resultados['cyclicMomentum']['test_acc'] = 0\n",
        "resultados['cyclicMomentum']['cost'] = [0] * epochs\n",
        "resultados['cyclicMomentum']['time'] = 0\n",
        "resultados['cyclicMomentum']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    cyclicMomentum_acc_list, cyclicMomentum_cost_list, cyclicMomentum_lr_list, cyclicMomentum_time, cyclicMomentum_acc, cyclicMomentum_epochs = CyclicMomentum()\n",
        "    resultados['cyclicMomentum']['val_acc_list'] = SumList(resultados['cyclicMomentum']['val_acc_list'], cyclicMomentum_acc_list)\n",
        "    resultados['cyclicMomentum']['test_acc'] += cyclicMomentum_acc\n",
        "    resultados['cyclicMomentum']['cost'] = SumList(resultados['cyclicMomentum']['cost'], cyclicMomentum_cost_list)\n",
        "    resultados['cyclicMomentum']['time'] += cyclicMomentum_time\n",
        "    resultados['cyclicMomentum']['epochs'] += cyclicMomentum_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['cyclicMomentum']['name'] = 'Ciclico Con Momentum'\n",
        "resultados['cyclicMomentum']['lr'] = cyclicMomentum_lr_list\n",
        "resultados['cyclicMomentum']['test_acc'] = resultados['cyclicMomentum']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['cyclicMomentum']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['cyclicMomentum']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['cyclicMomentum']['cost'] = DeleteZerosFromList(DivideList(resultados['cyclicMomentum']['cost'], MAX_ITERATIONS))\n",
        "resultados['cyclicMomentum']['time'] = resultados['cyclicMomentum']['time']\n",
        "resultados['cyclicMomentum']['epochs'] = resultados['cyclicMomentum']['epochs']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28gylOT-cNL5"
      },
      "source": [
        "## Cíclico aleatorio con Momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0_7uf-LUcQuE"
      },
      "outputs": [],
      "source": [
        "def CyclicGD_Momentum():\n",
        "    import random\n",
        "    modelRandomCyclic_Momentum= nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                        nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                        nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelRandomCyclic_Momentum.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    clr_fn = lambda x, y: random.uniform(x, y)\n",
        "\n",
        "    scheduler = CyclicLRGiselt_Denis(optimizer, base_lr=0.01, max_lr=0.1,step_size_up=2, scale_fn=clr_fn,scale_mode='chipichipi', cycle_momentum=False)\n",
        "\n",
        "    start.record()\n",
        "    random_cyclic_Momentum_acc_list, random_cyclic_Momentum_cost_list, random_cyclic_Momentum_lr_list, random_cyclic_epochs= train(modelRandomCyclic_Momentum, optimizer,scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    random_cyclic_Momentum_time = start.elapsed_time(end)\n",
        "\n",
        "    random_cyclic_Momentum_acc = accuracy(modelRandomCyclic_Momentum, x_test_tensor,  y_test_tensor, mb_size) \n",
        "\n",
        "    return random_cyclic_Momentum_acc_list, random_cyclic_Momentum_cost_list, random_cyclic_Momentum_lr_list, random_cyclic_Momentum_time, random_cyclic_Momentum_acc, random_cyclic_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Zikwca8lcrI-"
      },
      "outputs": [],
      "source": [
        "resultados['random_cyclic_Momentum'] = {}\n",
        "resultados['random_cyclic_Momentum']['val_acc_list'] = [0] * epochs\n",
        "resultados['random_cyclic_Momentum']['test_acc'] = 0\n",
        "resultados['random_cyclic_Momentum']['cost'] = [0] * epochs\n",
        "resultados['random_cyclic_Momentum']['time'] = 0\n",
        "resultados['random_cyclic_Momentum']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    random_cyclic_Momentum_acc_list, random_cyclic_Momentum_cost_list, random_cyclic_Momentum_lr_list, random_cyclic_Momentum_time, random_cyclic_Momentum_acc, random_cyclic_epochs = CyclicGD_Momentum()\n",
        "    a = SumList(resultados['random_cyclic_Momentum']['val_acc_list'], random_cyclic_Momentum_acc_list)\n",
        "    resultados['random_cyclic_Momentum']['val_acc_list'] = SumList(resultados['random_cyclic_Momentum']['val_acc_list'], random_cyclic_Momentum_acc_list)\n",
        "    resultados['random_cyclic_Momentum']['test_acc'] += random_cyclic_Momentum_acc\n",
        "    resultados['random_cyclic_Momentum']['cost'] = SumList(resultados['random_cyclic_Momentum']['cost'], random_cyclic_Momentum_cost_list)\n",
        "    resultados['random_cyclic_Momentum']['time'] += random_cyclic_Momentum_time\n",
        "    resultados['random_cyclic_Momentum']['epochs'] += random_cyclic_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['random_cyclic_Momentum']['name'] = 'Random Ciclico con Momentum'\n",
        "resultados['random_cyclic_Momentum']['lr'] = random_cyclic_Momentum_lr_list\n",
        "resultados['random_cyclic_Momentum']['test_acc'] = resultados['random_cyclic_Momentum']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['random_cyclic_Momentum']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['random_cyclic_Momentum']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['random_cyclic_Momentum']['cost'] = DeleteZerosFromList(DivideList(resultados['random_cyclic_Momentum']['cost'], MAX_ITERATIONS))\n",
        "resultados['random_cyclic_Momentum']['time'] = resultados['random_cyclic_Momentum']['time']\n",
        "resultados['random_cyclic_Momentum']['epochs'] = resultados['random_cyclic_Momentum']['epochs']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nuestro decreciente con Momentum"
      ],
      "metadata": {
        "id": "YH7jqvDOkCBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Our_DecayMomentum():\n",
        "    modelOurDecay = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelOurDecay.parameters(), lr=1, momentum=0.9)\n",
        "\n",
        "\n",
        "    clr_fn = lambda x, y: random.uniform(x, y)\n",
        "    scheduler = CyclicLRGiselt_Denis(optimizer, base_lr=0.0001, max_lr=0.1,scale_fn=clr_fn, step_size_up=1,scale_mode='decrecimiento', cycle_momentum=False)\n",
        "\n",
        "    start.record()\n",
        "    our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_epochs = train(modelOurDecay,optimizer, scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    our_decay_time = start.elapsed_time(end)\n",
        "\n",
        "    our_decay_acc = accuracy(modelOurDecay, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_time, our_decay_acc, our_decay_epochs"
      ],
      "metadata": {
        "id": "49T-DodfkFXZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultados['our_decay_momentum'] = {}\n",
        "resultados['our_decay_momentum']['val_acc_list'] = [0] * epochs\n",
        "resultados['our_decay_momentum']['test_acc'] = 0\n",
        "resultados['our_decay_momentum']['cost'] = [0] * epochs\n",
        "resultados['our_decay_momentum']['time'] = 0\n",
        "resultados['our_decay_momentum']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    our_decay_momentum_acc_list, our_decay_momentum_cost_list, our_decay_momentum_lr_list, our_decay_momentum_time, our_decay_momentum_acc, our_decay_momentum_epochs = Our_DecayMomentum()\n",
        "    resultados['our_decay_momentum']['val_acc_list'] = SumList(resultados['our_decay_momentum']['val_acc_list'], our_decay_momentum_acc_list)\n",
        "    resultados['our_decay_momentum']['test_acc'] += our_decay_momentum_acc\n",
        "    resultados['our_decay_momentum']['cost'] = SumList(resultados['our_decay_momentum']['cost'], our_decay_momentum_cost_list)\n",
        "    resultados['our_decay_momentum']['time'] += our_decay_momentum_time\n",
        "    resultados['our_decay_momentum']['epochs'] += our_decay_momentum_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['our_decay_momentum']['name'] = 'Nuestro decreciente con Momentum'\n",
        "resultados['our_decay_momentum']['lr'] = our_decay_momentum_lr_list\n",
        "resultados['our_decay_momentum']['test_acc'] = resultados['our_decay_momentum']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['our_decay_momentum']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['our_decay_momentum']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['our_decay_momentum']['cost'] = DeleteZerosFromList(DivideList(resultados['our_decay_momentum']['cost'], MAX_ITERATIONS))\n",
        "resultados['our_decay_momentum']['time'] = resultados['our_decay_momentum']['time'] \n",
        "resultados['our_decay_momentum']['epochs'] = resultados['our_decay_momentum']['epochs'] / MAX_ITERATIONS"
      ],
      "metadata": {
        "id": "pmom3iiZkJP6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8_h21aMdgsB"
      },
      "source": [
        "# Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJQ2_Q_q-s3a"
      },
      "source": [
        "Al guardar todos los resultados de cada método en sus respectivos diccionarios, se procede a crear un dataframe con estos resultados"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, _ in resultados.items():\n",
        "    if( resultados[key]['val_acc_list'][0] == 0 ):\n",
        "        continue\n",
        "    resultados[key]['val_acc_list'].insert(0,0)"
      ],
      "metadata": {
        "id": "niADA484gEHD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "7S5hiFtm-9xj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "resultados_df = pd.DataFrame(resultados.copy()).T\n",
        "\n",
        "resultados_df['val_acc'] = resultados_df.apply(lambda row: round(row['val_acc_list'][-1]* 100,2), axis=1)\n",
        "resultados_df['test_acc'] = resultados_df.apply(lambda row: round(row['test_acc']*100,2), axis=1)\n",
        "resultados_df['epochs'] = resultados_df.apply(lambda row: len(row['val_acc_list']), axis=1)\n",
        "resultados_df['val_acc'] = resultados_df.apply(lambda row: '{acc}%'.format(acc = row['val_acc']), axis=1)\n",
        "resultados_df['test_acc'] = resultados_df.apply(lambda row: '{acc}%'.format(acc = row['test_acc']), axis=1)\n",
        "\n",
        "resultados_df = resultados_df.sort_values(by=['epochs'],ascending=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Qelt3wIs3T"
      },
      "source": [
        "## Tiempos por epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "lruyulJ5C4SR",
        "outputId": "9ed53afb-0f8e-4f60-d2a8-b578f07d7d29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f087f73eee0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_b22f8_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"col_heading level0 col0\" >name</th>\n",
              "      <th class=\"col_heading level0 col1\" >test_acc</th>\n",
              "      <th class=\"col_heading level0 col2\" >val_acc</th>\n",
              "      <th class=\"col_heading level0 col3\" >epochs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_b22f8_row0_col0\" class=\"data row0 col0\" >Nuestro decreciente con Momentum</td>\n",
              "      <td id=\"T_b22f8_row0_col1\" class=\"data row0 col1\" >95.38%</td>\n",
              "      <td id=\"T_b22f8_row0_col2\" class=\"data row0 col2\" >95.56%</td>\n",
              "      <td id=\"T_b22f8_row0_col3\" class=\"data row0 col3\" >6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_b22f8_row1_col0\" class=\"data row1 col0\" >Ciclico Con Momentum</td>\n",
              "      <td id=\"T_b22f8_row1_col1\" class=\"data row1 col1\" >94.49%</td>\n",
              "      <td id=\"T_b22f8_row1_col2\" class=\"data row1 col2\" >95.25%</td>\n",
              "      <td id=\"T_b22f8_row1_col3\" class=\"data row1 col3\" >8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_b22f8_row2_col0\" class=\"data row2 col0\" >Random Ciclico con Momentum</td>\n",
              "      <td id=\"T_b22f8_row2_col1\" class=\"data row2 col1\" >95.02%</td>\n",
              "      <td id=\"T_b22f8_row2_col2\" class=\"data row2 col2\" >95.62%</td>\n",
              "      <td id=\"T_b22f8_row2_col3\" class=\"data row2 col3\" >8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_b22f8_row3_col0\" class=\"data row3 col0\" >Nuestro decreciente</td>\n",
              "      <td id=\"T_b22f8_row3_col1\" class=\"data row3 col1\" >94.42%</td>\n",
              "      <td id=\"T_b22f8_row3_col2\" class=\"data row3 col2\" >95.33%</td>\n",
              "      <td id=\"T_b22f8_row3_col3\" class=\"data row3 col3\" >24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_b22f8_row4_col0\" class=\"data row4 col0\" >Momentum</td>\n",
              "      <td id=\"T_b22f8_row4_col1\" class=\"data row4 col1\" >94.51%</td>\n",
              "      <td id=\"T_b22f8_row4_col2\" class=\"data row4 col2\" >95.24%</td>\n",
              "      <td id=\"T_b22f8_row4_col3\" class=\"data row4 col3\" >25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_b22f8_row5_col0\" class=\"data row5 col0\" >Random Ciclico</td>\n",
              "      <td id=\"T_b22f8_row5_col1\" class=\"data row5 col1\" >94.07%</td>\n",
              "      <td id=\"T_b22f8_row5_col2\" class=\"data row5 col2\" >95.07%</td>\n",
              "      <td id=\"T_b22f8_row5_col3\" class=\"data row5 col3\" >38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_b22f8_row6_col0\" class=\"data row6 col0\" >Ciclico</td>\n",
              "      <td id=\"T_b22f8_row6_col1\" class=\"data row6 col1\" >94.27%</td>\n",
              "      <td id=\"T_b22f8_row6_col2\" class=\"data row6 col2\" >95.11%</td>\n",
              "      <td id=\"T_b22f8_row6_col3\" class=\"data row6 col3\" >39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "resultados_df[['name','test_acc','val_acc', 'epochs']].style.hide_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhAah9L2cmoj"
      },
      "source": [
        "## Convergencia en iteraciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "bP3C7xC1vql8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "1688c272-67f0-403d-8d41-cc5acc778c45"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-7b7df25b4b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#Precisiones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresultados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     ax[1,1].plot(range(len(resultados[k1]['val_acc_list'])),\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mresultados\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         label='{name} = {acc}%'.format(name= resultados[k1]['name'],\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x360 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAEzCAYAAAC7Xe1fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPp0lEQVR4nO3cf6jdd33H8dfbZp3MVR02gjSpVpZOMx3YXYpDmB26kXbQ/uEmLZTNUQw6KwNl0OHopP7lZA6EbC4wqQpaq3+MgJHCXKUgRptSrbalEqtbU2WNWv1HtJa998c5brfXpPfk9nty7+f08YDA+fHN/b45uW+e95xzc6q7AwAje852DwAAz5SYATA8MQNgeGIGwPDEDIDhiRkAw9s0ZlX1kap6rKq+cYb7q6o+VFUnquq+qrps+jFhddgpmN4iz8xuTXLgae6/Msm++Z+DSf75mY8FK+3W2CmY1KYx6+67kvzwaQ65JsnHeuZYkhdW1UumGhBWjZ2C6U3xntlFSR5Zd/3k/DZga+wUnKVd5/JkVXUws5dN8rznPe93X/GKV5zL08Pk7rnnnu939+7tOr+dYtVsdaemiNmjSfauu75nftsv6e7DSQ4nydraWh8/fnyC08P2qar/XMKXtVM8a211p6Z4mfFIkj+b/wbWa5P8uLu/N8HXhWcrOwVnadNnZlX1ySRXJLmwqk4m+bskv5Ik3f3hJEeTXJXkRJKfJPmLZQ0Lq8BOwfQ2jVl3X7fJ/Z3kHZNNBCvOTsH0fAIIAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHgLxayqDlTVQ1V1oqpuOs39F1fVnVV1b1XdV1VXTT8qrA47BdPaNGZVdV6SQ0muTLI/yXVVtX/DYX+b5Pbufk2Sa5P809SDwqqwUzC9RZ6ZXZ7kRHc/3N1PJLktyTUbjukkz59ffkGS7043IqwcOwUTWyRmFyV5ZN31k/Pb1ntvkuur6mSSo0neebovVFUHq+p4VR0/derUFsaFlWCnYGJT/QLIdUlu7e49Sa5K8vGq+qWv3d2Hu3utu9d279490alhJdkpOAuLxOzRJHvXXd8zv229G5LcniTd/aUkz01y4RQDwgqyUzCxRWJ2d5J9VXVJVZ2f2ZvRRzYc819J3pAkVfXKzBbPax5wenYKJrZpzLr7ySQ3JrkjyYOZ/YbV/VV1S1VdPT/s3UneWlVfS/LJJG/p7l7W0DAyOwXT27XIQd19NLM3odffdvO6yw8ked20o8HqslMwLZ8AAsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADG+hmFXVgap6qKpOVNVNZzjmzVX1QFXdX1WfmHZMWC12Cqa1a7MDquq8JIeS/GGSk0nurqoj3f3AumP2JfmbJK/r7ser6sXLGhhGZ6dgeos8M7s8yYnufri7n0hyW5JrNhzz1iSHuvvxJOnux6YdE1aKnYKJLRKzi5I8su76yflt612a5NKq+mJVHauqA1MNCCvITsHENn2Z8Sy+zr4kVyTZk+Suqnp1d/9o/UFVdTDJwSS5+OKLJzo1rCQ7BWdhkWdmjybZu+76nvlt651McqS7f97d307yzcwW8Sm6+3B3r3X32u7du7c6M4zOTsHEFonZ3Un2VdUlVXV+kmuTHNlwzL9l9hNkqurCzF4ieXi6MWGl2CmY2KYx6+4nk9yY5I4kDya5vbvvr6pbqurq+WF3JPlBVT2Q5M4kf93dP1jW0DAyOwXTq+7elhOvra318ePHt+XcMJWquqe717Z7jsROsRq2ulM+AQSA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4YkZAMMTMwCGJ2YADE/MABjeQjGrqgNV9VBVnaiqm57muDdVVVfV2nQjwuqxUzCtTWNWVeclOZTkyiT7k1xXVftPc9wFSf4qyZenHhJWiZ2C6S3yzOzyJCe6++HufiLJbUmuOc1x70vy/iQ/nXA+WEV2Cia2SMwuSvLIuusn57f9n6q6LMne7v7shLPBqrJTMLFn/AsgVfWcJB9M8u4Fjj1YVcer6vipU6ee6alhJdkpOHuLxOzRJHvXXd8zv+0XLkjyqiRfqKrvJHltkiOne8O6uw9391p3r+3evXvrU8PY7BRMbJGY3Z1kX1VdUlXnJ7k2yZFf3NndP+7uC7v7Zd39siTHklzd3ceXMjGMz07BxDaNWXc/meTGJHckeTDJ7d19f1XdUlVXL3tAWDV2Cqa3a5GDuvtokqMbbrv5DMde8czHgtVmp2BaPgEEgOGJGQDDEzMAhidmAAxPzAAYnpgBMDwxA2B4YgbA8MQMgOGJGQDDEzMAhidmAAxPzAAYnpgBMDwxA2B4YgbA8MQMgOGJGQDDEzMAhidmAAxPzAAYnpgBMDwxA2B4YgbA8MQMgOGJGQDDEzMAhidmAAxPzAAYnpgBMDwxA2B4YgbA8MQMgOGJGQDDEzMAhidmAAxPzAAYnpgBMDwxA2B4YgbA8MQMgOGJGQDDEzMAhidmAAxPzAAYnpgBMLyFYlZVB6rqoao6UVU3neb+d1XVA1V1X1V9vqpeOv2osDrsFExr05hV1XlJDiW5Msn+JNdV1f4Nh92bZK27fyfJZ5L8/dSDwqqwUzC9RZ6ZXZ7kRHc/3N1PJLktyTXrD+juO7v7J/Orx5LsmXZMWCl2Cia2SMwuSvLIuusn57edyQ1JPne6O6rqYFUdr6rjp06dWnxKWC12CiY26S+AVNX1SdaSfOB093f34e5e6+613bt3T3lqWEl2Chaza4FjHk2yd931PfPbnqKq3pjkPUle390/m2Y8WEl2Cia2yDOzu5Psq6pLqur8JNcmObL+gKp6TZJ/SXJ1dz82/ZiwUuwUTGzTmHX3k0luTHJHkgeT3N7d91fVLVV19fywDyT59SSfrqqvVtWRM3w5eNazUzC9RV5mTHcfTXJ0w203r7v8xonngpVmp2BaPgEEgOGJGQDDEzMAhidmAAxPzAAYnpgBMDwxA2B4YgbA8MQMgOGJGQDDEzMAhidmAAxPzAAYnpgBMDwxA2B4YgbA8MQMgOGJGQDDEzMAhidmAAxPzAAYnpgBMDwxA2B4YgbA8MQMgOGJGQDDEzMAhidmAAxPzAAYnpgBMDwxA2B4YgbA8MQMgOGJGQDDEzMAhidmAAxPzAAYnpgBMDwxA2B4YgbA8MQMgOGJGQDDEzMAhidmAAxPzAAY3kIxq6oDVfVQVZ2oqptOc/+vVtWn5vd/uapeNvmksELsFExr05hV1XlJDiW5Msn+JNdV1f4Nh92Q5PHu/s0k/5jk/VMPCqvCTsH0FnlmdnmSE939cHc/keS2JNdsOOaaJB+dX/5MkjdUVU03JqwUOwUTWyRmFyV5ZN31k/PbTntMdz+Z5MdJXjTFgLCC7BRMbNe5PFlVHUxycH71Z1X1jXN5/k1cmOT72z3E3E6aJdlZ8+ykWZLkt7bz5HZqYTtplmRnzbOTZkm2uFOLxOzRJHvXXd8zv+10x5ysql1JXpDkBxu/UHcfTnI4SarqeHevbWXoZdhJ8+ykWZKdNc9OmiWZzbOFv2anzrGdNEuys+bZSbMkW96phV5mvDvJvqq6pKrOT3JtkiMbjjmS5M/nl/8kyX90d29lIHgWsFMwsU2fmXX3k1V1Y5I7kpyX5CPdfX9V3ZLkeHcfSfKvST5eVSeS/DCz5QROw07B9BZ6z6y7jyY5uuG2m9dd/mmSPz3Lcx8+y+OXbSfNs5NmSXbWPDtplmSL89ipc24nzZLsrHl20izJFucpr1wAMDofZwXA8JYes530sT0LzPKuqnqgqu6rqs9X1UuXNcsi86w77k1V1VW1tN84WmSWqnrz/PG5v6o+saxZFpmnqi6uqjur6t75v9dVS5zlI1X12Jl+7b1mPjSf9b6qumxZs8zPZ6e2OM+64+zUL98/9k5199L+ZPbm9reSvDzJ+Um+lmT/hmP+MsmH55evTfKpbZzlD5L82vzy25c1y6LzzI+7IMldSY4lWdvGx2ZfknuT/Mb8+ou3+fvmcJK3zy/vT/KdJc7z+0kuS/KNM9x/VZLPJakkr03y5W1+bOyUndrKPEPv1LKfme2kj+3ZdJbuvrO7fzK/eiyz//+zLIs8Nknyvsw+l++n2zzLW5Mc6u7Hk6S7H9vmeTrJ8+eXX5Dku8saprvvyuw3Cs/kmiQf65ljSV5YVS9Z0jh26hnMM2enVnCnlh2znfSxPYvMst4Nmf1ksCybzjN/ar23uz+7xDkWmiXJpUkuraovVtWxqjqwzfO8N8n1VXUys98KfOcS59nM2X5vLftcduoM89ipp53nvRl4p87px1mNoqquT7KW5PXbOMNzknwwyVu2a4YNdmX2ssgVmf10fVdVvbq7f7RN81yX5Nbu/oeq+r3M/k/Wq7r7f7ZpHp6GnTotOzWhZT8zO5uP7Uk9zcf2nKNZUlVvTPKeJFd398+WMMei81yQ5FVJvlBV38nsdeMjS3rDepHH5mSSI9398+7+dpJvZraIy7DIPDckuT1JuvtLSZ6b2WfMbYeFvrfO4bns1OnnsVNPP8/YO7WsN/jmb+LtSvJwkkvy/286/vaGY96Rp75Zffs2zvKazN4k3bfMx2XReTYc/4Us783qRR6bA0k+Or98YWYvAbxoG+f5XJK3zC+/MrPX92uJ/14vy5nfrP7jPPXN6q9s5/eNnbJTW5xn6J1a6jfXfKirMvuJ41tJ3jO/7ZbMfkpLZvX/dJITSb6S5OXbOMu/J/nvJF+d/zmynY/NhmOXtngLPjaV2Us0DyT5epJrt/n7Zn+SL86X8qtJ/miJs3wyyfeS/Dyzn6ZvSPK2JG9b99gcms/69WX+Oy342NgpO7WVeYbeKZ8AAsDwfAIIAMMTMwCGJ2YADE/MABiemAEwPDEDYHhiBsDwxAyA4f0v43iO/f/aDyMAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from matplotlib.ticker import MultipleLocator\n",
        "\n",
        "bbox = dict(boxstyle =\"round\", fc =\"1\")\n",
        "arrowprops = dict(\n",
        "    arrowstyle = \"->\")\n",
        "offset = 72\n",
        "\n",
        "x_axis = range(100)\n",
        "y_axis = np.linspace(0,5,10)\n",
        "\n",
        "fig, ax = plt.subplots(1,2,figsize=(7, 5))\n",
        "\n",
        "\n",
        "#Precisiones\n",
        "for k1, _ in resultados.items():\n",
        "    ax[1,1].plot(range(len(resultados[k1]['val_acc_list'])),\n",
        "        resultados[k1]['val_acc_list'],\n",
        "        label='{name} = {acc}%'.format(name= resultados[k1]['name'],\n",
        "                                       acc = round(resultados[k1]['val_acc_list'][-1] * 100 , 2)))\n",
        "\n",
        "plt.title('Traza de la precisión', fontsize = 18)\n",
        "\n",
        "ax[1,1].set_xlabel('# Epochs', fontsize = 18)\n",
        "ax[1,1].set_ylabel('Train Accuracy', fontsize = 18)\n",
        "ax[1,1].spines['top'].set_visible(False)\n",
        "ax[1,1].spines['right'].set_visible(False)\n",
        "ax[1,1].xaxis.set_major_locator(MultipleLocator(10))\n",
        "ax[1,1].xaxis.set_minor_locator(MultipleLocator(1))\n",
        "ax[1,1].legend()\n",
        "\n",
        "#Costos\n",
        "for k1, _ in resultados.items():\n",
        "    ax[1,2].plot(range(len(resultados[k1]['cost'])),\n",
        "        resultados[k1]['cost'],\n",
        "        label='{name} = {acc}%'.format(name= resultados[k1]['name'],\n",
        "                                       acc = round(resultados[k1]['cost'][-1] * 100 , 2)))\n",
        "\n",
        "plt.title('Traza de los costos', fontsize = 18)\n",
        "\n",
        "ax[1,2].set_xlabel('# Epochs', fontsize = 18)\n",
        "ax[1,2].set_ylabel('Costo de pérdida', fontsize = 18)\n",
        "ax[1,2].spines['top'].set_visible(False)\n",
        "ax[1,2].spines['right'].set_visible(False)\n",
        "ax[1,2].xaxis.set_major_locator(MultipleLocator(10))\n",
        "ax[1,2].xaxis.set_minor_locator(MultipleLocator(1))\n",
        "ax[1,2].legend()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "ccfea65839ea7b4cb7611917847ae955f8a5d3f7496a05c88f3bf628abe673bf"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}