{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5BsElnuhMMO"
      },
      "source": [
        "Para realizar las importaciones necesarias, se debe agregar la referencia del repositorio de Github en la libreta para que tenga a su disposici√≥n los archivos del mismo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQc6T5wylcvf",
        "outputId": "367c215c-2aac-4ccc-d9c9-60d56915c7ff"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/DenisPerez/NN_Tests_DG.git\n",
        "%cd NN_Tests_DG\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDBoEOe9hDan"
      },
      "source": [
        "Se realizan las importaciones requeridas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4gP7LPQTtg7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from get_images import get_images\n",
        "\n",
        "# PyTorch \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-mULGvWTthB"
      },
      "source": [
        "# Get Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vdIQd58TthD"
      },
      "outputs": [],
      "source": [
        "MNIST_PATH = './Dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slQPSqQeTthE"
      },
      "outputs": [],
      "source": [
        "x_train_num, y_train_num, x_test_num, y_test_num = get_images(MNIST_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFnaRwd-TthE"
      },
      "outputs": [],
      "source": [
        "#Training set\n",
        "x_train = x_train_num[:50000].reshape(50000,-1).astype(np.float32)/255 ##Convert the traint set into a (50000, 28x28) matrix normalized\n",
        "y_train = y_train_num[:50000].reshape(50000,1)\n",
        "\n",
        "##Validation set\n",
        "x_val = x_train_num[50000:].reshape(10000,-1).astype(np.float32)/255\n",
        "y_val = y_train_num[50000:].reshape(10000,1)\n",
        "\n",
        "##Test set\n",
        "x_test = x_test_num.copy().reshape(10000,-1).astype(np.float32)/255\n",
        "y_test = y_test_num.copy().reshape(10000,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZz8hG0OTthF"
      },
      "source": [
        "## Normalize images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kFtDZs6TthG"
      },
      "outputs": [],
      "source": [
        "def normalise(x_mean, x_std, x_data):\n",
        "    return (x_data - x_mean) / x_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYhJenIZTthG"
      },
      "outputs": [],
      "source": [
        "x_mean = x_train.mean()\n",
        "x_std = x_train.std()\n",
        "\n",
        "x_train = normalise(x_mean, x_std, x_train)\n",
        "x_val = normalise(x_mean, x_std, x_val)\n",
        "x_test = normalise(x_mean, x_std, x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRx3THErTthH",
        "outputId": "f4df2cd7-fc89-41a1-88d3-7f1b0d0bd422"
      },
      "outputs": [],
      "source": [
        "x_train.mean(), x_train.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMAv3EVXTthJ"
      },
      "source": [
        "## Show Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHHAe687TthJ",
        "outputId": "70c463fb-0845-42e7-9792-840175f67609"
      },
      "outputs": [],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_FPmhofTthK",
        "outputId": "ecd91d05-a370-4a1b-f095-edeaafd698d6"
      },
      "outputs": [],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4HfkSTqTthK"
      },
      "outputs": [],
      "source": [
        "def plot_number(image):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7TQtWTpZTthL",
        "outputId": "f8d9a4ed-6388-4e06-885e-f3a4f327a87f"
      },
      "outputs": [],
      "source": [
        "rnd_idx = np.random.randint(len(y_test))\n",
        "print(f'La imagen muestreada representa un: {y_test[rnd_idx, 0]}')\n",
        "plot_number(x_test_num[rnd_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVo-PF6cTthM"
      },
      "source": [
        "## Create Mini Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym639R_PTthM"
      },
      "outputs": [],
      "source": [
        "def create_minibatches(x, y, mb_size, shuffle = True):\n",
        "    '''\n",
        "    x  #muestras, input_layer\n",
        "    y #muestras, 1\n",
        "    '''\n",
        "    assert x.shape[0] == y.shape[0], 'Error en cantidad de muestras'\n",
        "    total_data = x.shape[0]\n",
        "    if shuffle: \n",
        "        idxs = np.arange(total_data, dtype=float)\n",
        "        np.random.shuffle(idxs)\n",
        "        x = x[idxs]\n",
        "        y = y[idxs]  \n",
        "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5Ypqe6nTthN"
      },
      "source": [
        "# Pytorch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6quE_eQ9TthO"
      },
      "outputs": [],
      "source": [
        "x_train_tensor = torch.Tensor(x_train.copy())\n",
        "y_train_tensor = torch.Tensor(y_train.copy())\n",
        "\n",
        "x_val_tensor = torch.Tensor(x_val.copy())\n",
        "y_val_tensor = torch.Tensor(y_val.copy())\n",
        "\n",
        "x_test_tensor = torch.Tensor(x_test.copy())\n",
        "y_test_tensor = torch.Tensor(y_test.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqPgXB1DTthP"
      },
      "source": [
        "## Use GPU when available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1oc0PTWTthQ",
        "outputId": "31d7825d-de86-4268-b130-1b4e22f6ea63"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZebY-iBHTthQ",
        "outputId": "ff34b9b9-df60-4819-a05b-e7f97cd04088"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(f'Estamos usando: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L66abJ9WTthR"
      },
      "source": [
        "# Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt6iiZyuTthR"
      },
      "outputs": [],
      "source": [
        "def accuracy(model: nn.Sequential, x: torch.tensor, y: torch.tensor, mb_size: int):\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    model.eval()\n",
        "    model = model.to(device=device)\n",
        "    with torch.no_grad():\n",
        "        for (xi, yi) in create_minibatches(x, y, mb_size):\n",
        "            xi = xi.to(device=device, dtype = torch.float32)\n",
        "            yi = yi.to(device=device, dtype = torch.long)\n",
        "            scores = model(xi) # mb_size, 10\n",
        "            _, pred = scores.max(dim=1) #pred shape (mb_size )\n",
        "            num_correct += (pred == yi.squeeze()).sum() # pred shape (mb_size), yi shape (mb_size, 1)\n",
        "            num_total += pred.size(0)\n",
        "\n",
        "            return float(num_correct)/num_total  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR4DuKf3TthR"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtRDD6Nu0507"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, scheduler: None, mb_size):\n",
        "    model = model.to(device=device)\n",
        "    epoch_acc = 0.0\n",
        "    i = 0\n",
        "    #plot lists\n",
        "    acc_list = [0.0]\n",
        "    cost_list = [0.0]\n",
        "    lr_list = [0.0]\n",
        "    while (epoch_acc < 0.95 and i < 100):\n",
        "        for (xi, yi) in create_minibatches(x_train_tensor, y_train_tensor, mb_size):\n",
        "            model.train()\n",
        "            xi = xi.to(device=device, dtype=torch.float32)\n",
        "            yi = yi.to(device=device, dtype=torch.long)\n",
        "            scores = model(xi)\n",
        "            # cost function\n",
        "            cost = F.cross_entropy(input= scores, target=yi.squeeze())\n",
        "            optimizer.zero_grad()\n",
        "            cost.backward()\n",
        "            for name, param in model.named_parameters():\n",
        "              ik = str(name)+'_'+str(i)\n",
        "              prev_ik = str(name)+'_'+str(i-1)\n",
        "            optimizer.step()\n",
        "        if (scheduler != None):\n",
        "          scheduler.step()\n",
        "          lr = scheduler.get_last_lr()\n",
        "          lr_list.append(lr[0])\n",
        "        else: \n",
        "          lr = optimizer.param_groups[0]['lr']\n",
        "          lr_list.append(lr)\n",
        "        i+=1\n",
        "        epoch_acc = accuracy(model, x_val_tensor, y_val_tensor, mb_size)\n",
        "        epoch_cost = cost.item()\n",
        "\n",
        "        #append\n",
        "        acc_list.append(epoch_acc)\n",
        "        cost_list.append(epoch_cost)\n",
        "        print(f'Epoch: {len(acc_list) - 1}, learning_rate:{lr},costo: {epoch_cost}, accuracy: {epoch_acc}')\n",
        "    return acc_list, cost_list, lr_list, i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgICBmLcfGaN"
      },
      "source": [
        "## List Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CrTwzYWfGaO"
      },
      "outputs": [],
      "source": [
        "def SumList(first: list, second: list) -> list:\n",
        "    return [x + y for x, y in zip(first[::-1], second[::-1])][::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtgpbpnufGaO"
      },
      "outputs": [],
      "source": [
        "def DivideList(dic_list: list, number: int) -> list:\n",
        "    return [x / number for x in dic_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIPWRLiJfGaO"
      },
      "outputs": [],
      "source": [
        "def DeleteZerosFromList(dic_list: list) -> list:\n",
        "    return list(filter(lambda num: num != 0, dic_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "015SGTIaTthS"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ2qa3c-eS1L"
      },
      "source": [
        "### Variables globales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBDuey1VQTm4"
      },
      "outputs": [],
      "source": [
        "MAX_ITERATIONS = 2\n",
        "\n",
        "layer1 = 1000 \n",
        "layer2 = 1000\n",
        "lr = 1e-2\n",
        "epochs = 100\n",
        "mb_size = 4096\n",
        "input_layer = 784\n",
        "first_i = 0\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "resultados = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKKLswGF1fN2"
      },
      "source": [
        "## Fija"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVrJ7coqfGaQ"
      },
      "outputs": [],
      "source": [
        "def Fixed():\n",
        "    modelFixed = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelFixed.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "    start.record()\n",
        "    fixed_acc_list, fixed_cost_list, fixed_lr_list, fixed_epochs = train(modelFixed, optimizer,None, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    fixed_time = start.elapsed_time(end)\n",
        "\n",
        "    fixed_acc = accuracy(modelFixed, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return fixed_acc_list, fixed_cost_list, fixed_lr_list, fixed_time, fixed_acc, fixed_epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "ipwfv9PgTthT",
        "outputId": "de792190-2113-4a49-b544-b3f837647063"
      },
      "outputs": [],
      "source": [
        "\n",
        "resultados['fixed'] = {}\n",
        "resultados['fixed']['val_acc_list'] = [0] * epochs\n",
        "resultados['fixed']['test_acc'] = 0\n",
        "resultados['fixed']['cost'] = [0] * epochs\n",
        "resultados['fixed']['time'] = 0\n",
        "resultados['fixed']['epochs'] = 0\n",
        "\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    fixed_acc_list, fixed_cost_list, fixed_lr_list, fixed_time, fixed_acc, fixed_epochs = Fixed()\n",
        "    resultados['fixed']['val_acc_list'] = SumList(resultados['fixed']['val_acc_list'], fixed_acc_list)\n",
        "    resultados['fixed']['test_acc'] += fixed_acc\n",
        "    resultados['fixed']['cost'] = SumList(resultados['fixed']['cost'], fixed_cost_list)\n",
        "    resultados['fixed']['time'] += fixed_time\n",
        "    resultados['fixed']['epochs'] += fixed_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['fixed']['name'] = 'Fijo'\n",
        "resultados['fixed']['lr'] = fixed_lr_list\n",
        "resultados['fixed']['test_acc'] = resultados['fixed']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['fixed']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['fixed']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['fixed']['cost'] = DeleteZerosFromList(DivideList(resultados['fixed']['cost'], MAX_ITERATIONS))\n",
        "resultados['fixed']['time'] = resultados['fixed']['time']\n",
        "resultados['fixed']['epochs'] = resultados['fixed']['epochs'] / MAX_ITERATIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EybNcRix9Krm"
      },
      "source": [
        "## Decreciente (Pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkQsuESPfGaR"
      },
      "outputs": [],
      "source": [
        "def Decay():\n",
        "    modelDecay = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelDecay.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1, last_epoch=-1, verbose=False)\n",
        "\n",
        "    start.record()\n",
        "    decay_acc_list, decay_cost_list, decay_lr_list, decay_epochs = train(modelDecay,optimizer, scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    decay_time = start.elapsed_time(end)\n",
        "\n",
        "    decay_acc = accuracy(modelDecay, x_test_tensor,  y_test_tensor, mb_size)\n",
        "    \n",
        "    return decay_acc_list, decay_cost_list, decay_lr_list, decay_time, decay_acc, decay_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "C69-CXmo9N3L",
        "outputId": "cd3d4eca-fbe4-4c77-8c31-0426fc08d74c"
      },
      "outputs": [],
      "source": [
        "\n",
        "resultados['decay'] = {}\n",
        "resultados['decay']['val_acc_list'] = [0] * epochs\n",
        "resultados['decay']['test_acc'] = 0\n",
        "resultados['decay']['cost'] = [0] * epochs\n",
        "resultados['decay']['time'] = 0\n",
        "resultados['decay']['epochs'] = 0\n",
        "\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    decay_acc_list, decay_cost_list, decay_lr_list, decay_time, decay_acc, decay_epochs = Decay()\n",
        "    resultados['decay']['val_acc_list'] = SumList(resultados['decay']['val_acc_list'], decay_acc_list)\n",
        "    resultados['decay']['test_acc'] += decay_acc\n",
        "    resultados['decay']['cost'] = SumList(resultados['decay']['cost'], decay_cost_list)\n",
        "    resultados['decay']['time'] += decay_time\n",
        "    resultados['decay']['epochs'] += decay_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['decay']['name'] = 'Decreciente'\n",
        "resultados['decay']['lr'] = decay_lr_list\n",
        "resultados['decay']['test_acc'] = resultados['decay']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['decay']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['decay']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['decay']['cost'] = DeleteZerosFromList(DivideList(resultados['decay']['cost'], MAX_ITERATIONS))\n",
        "resultados['decay']['time'] = resultados['decay']['time']\n",
        "resultados['decay']['epochs'] = resultados['decay']['epochs'] / MAX_ITERATIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mAWn11Vmc6-"
      },
      "source": [
        "## Cyclic Vanilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQlDjhUFfGaS"
      },
      "outputs": [],
      "source": [
        "def Cyclic():\n",
        "    modelCyclic = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelCyclic.parameters(), lr=lr)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1,step_size_up=3,cycle_momentum=False )\n",
        "\n",
        "    start.record()\n",
        "    cyclic_acc_list, cyclic_cost_list, cyclic_lr_list, cyclic_epochs= train(modelCyclic, optimizer,scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    cyclic_time = start.elapsed_time(end)\n",
        "\n",
        "    cyclic_acc = accuracy(modelCyclic, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return cyclic_acc_list, cyclic_cost_list, cyclic_lr_list, cyclic_time, cyclic_acc, cyclic_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWp659FGmhDK",
        "outputId": "9546771c-938c-4444-b5d4-f76a51a59b05"
      },
      "outputs": [],
      "source": [
        "resultados['cyclic'] = {}\n",
        "resultados['cyclic']['val_acc_list'] = [0]* epochs\n",
        "resultados['cyclic']['test_acc'] = 0\n",
        "resultados['cyclic']['cost'] = [0] * epochs\n",
        "resultados['cyclic']['time'] = 0\n",
        "resultados['cyclic']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    cyclic_acc_list, cyclic_cost_list, cyclic_lr_list, cyclic_time, cyclic_acc, cyclic_epochs = Cyclic()\n",
        "    resultados['cyclic']['val_acc_list'] = SumList(resultados['cyclic']['val_acc_list'], cyclic_acc_list)\n",
        "    resultados['cyclic']['test_acc'] += cyclic_acc\n",
        "    resultados['cyclic']['cost'] = SumList(resultados['cyclic']['cost'], cyclic_cost_list)\n",
        "    resultados['cyclic']['time'] += cyclic_time\n",
        "    resultados['cyclic']['epochs'] += cyclic_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['cyclic']['name'] = 'Ciclico'\n",
        "resultados['cyclic']['lr'] = cyclic_lr_list\n",
        "resultados['cyclic']['test_acc'] = resultados['cyclic']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['cyclic']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['cyclic']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['cyclic']['cost'] = DeleteZerosFromList(DivideList(resultados['cyclic']['cost'], MAX_ITERATIONS))\n",
        "resultados['cyclic']['time'] = resultados['cyclic']['time']\n",
        "resultados['cyclic']['epochs'] = resultados['cyclic']['epochs'] / MAX_ITERATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj0arCxmZdxw"
      },
      "source": [
        "## Cyclic Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vrI2SE_3zaR"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "\n",
        "import types\n",
        "import math\n",
        "from torch._six import inf\n",
        "from functools import wraps\n",
        "import warnings\n",
        "import weakref\n",
        "import random\n",
        "\n",
        "class _LRSchedulerGiselt_Denis(object):\n",
        "\n",
        "    def __init__(self, optimizer, last_epoch=-1, verbose=False):\n",
        "\n",
        "        # Attach optimizer\n",
        "        if not isinstance(optimizer, Optimizer):\n",
        "            raise TypeError('{} is not an Optimizer'.format(\n",
        "                type(optimizer).__name__))\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Initialize epoch and base learning rates\n",
        "        if last_epoch == -1:\n",
        "            for group in optimizer.param_groups:\n",
        "                group.setdefault('initial_lr', group['lr'])\n",
        "        else:\n",
        "            for i, group in enumerate(optimizer.param_groups):\n",
        "                if 'initial_lr' not in group:\n",
        "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
        "                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
        "        self.base_lrs = [group['initial_lr'] for group in optimizer.param_groups]\n",
        "        self.last_epoch = last_epoch\n",
        "\n",
        "        # Following https://github.com/pytorch/pytorch/issues/20124\n",
        "        # We would like to ensure that `lr_scheduler.step()` is called after\n",
        "        # `optimizer.step()`\n",
        "        def with_counter(method):\n",
        "            if getattr(method, '_with_counter', False):\n",
        "                # `optimizer.step()` has already been replaced, return.\n",
        "                return method\n",
        "\n",
        "            # Keep a weak reference to the optimizer instance to prevent\n",
        "            # cyclic references.\n",
        "            instance_ref = weakref.ref(method.__self__)\n",
        "            # Get the unbound method for the same purpose.\n",
        "            func = method.__func__\n",
        "            cls = instance_ref().__class__\n",
        "            del method\n",
        "\n",
        "            @wraps(func)\n",
        "            def wrapper(*args, **kwargs):\n",
        "                instance = instance_ref()\n",
        "                instance._step_count += 1\n",
        "                wrapped = func.__get__(instance, cls)\n",
        "                return wrapped(*args, **kwargs)\n",
        "\n",
        "            # Note that the returned function here is no longer a bound method,\n",
        "            # so attributes like `__func__` and `__self__` no longer exist.\n",
        "            wrapper._with_counter = True\n",
        "            return wrapper\n",
        "\n",
        "        self.optimizer.step = with_counter(self.optimizer.step)\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self._initial_step()\n",
        "\n",
        "    def _initial_step(self):\n",
        "        \"\"\"Initialize step counts and performs a step\"\"\"\n",
        "        self.optimizer._step_count = 0\n",
        "        self._step_count = 0\n",
        "        self.step()\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
        "\n",
        "        It contains an entry for every variable in self.__dict__ which\n",
        "        is not the optimizer.\n",
        "        \"\"\"\n",
        "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"\"\"Loads the schedulers state.\n",
        "\n",
        "        Args:\n",
        "            state_dict (dict): scheduler state. Should be an object returned\n",
        "                from a call to :meth:`state_dict`.\n",
        "        \"\"\"\n",
        "        self.__dict__.update(state_dict)\n",
        "\n",
        "    def get_last_lr(self):\n",
        "        \"\"\" Return last computed learning rate by current scheduler.\n",
        "        \"\"\"\n",
        "        return self._last_lr\n",
        "\n",
        "    def get_lr(self):\n",
        "        # Compute learning rate using chainable form of the scheduler\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def print_lr(self, is_verbose, group, lr, epoch=None):\n",
        "        \"\"\"Display the current learning rate.\n",
        "        \"\"\"\n",
        "        if is_verbose:\n",
        "            if epoch is None:\n",
        "                print('Adjusting learning rate'\n",
        "                      ' of group {} to {:.4e}.'.format(group, lr))\n",
        "            else:\n",
        "                epoch_str = (\"%.2f\" if isinstance(epoch, float) else\n",
        "                             \"%.5d\") % epoch\n",
        "                print('Epoch {}: adjusting learning rate'\n",
        "                      ' of group {} to {:.4e}.'.format(epoch_str, group, lr))\n",
        "\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        # Raise a warning if old pattern is detected\n",
        "        # https://github.com/pytorch/pytorch/issues/20124\n",
        "        if self._step_count == 1:\n",
        "            if not hasattr(self.optimizer.step, \"_with_counter\"):\n",
        "                warnings.warn(\"Seems like `optimizer.step()` has been overridden after learning rate scheduler \"\n",
        "                              \"initialization. Please, make sure to call `optimizer.step()` before \"\n",
        "                              \"`lr_scheduler.step()`. See more details at \"\n",
        "                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
        "\n",
        "            # Just check if there were two first lr_scheduler.step() calls before optimizer.step()\n",
        "            elif self.optimizer._step_count < 1:\n",
        "                warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
        "                              \"In PyTorch 1.1.0 and later, you should call them in the opposite order: \"\n",
        "                              \"`optimizer.step()` before `lr_scheduler.step()`.  Failure to do this \"\n",
        "                              \"will result in PyTorch skipping the first value of the learning rate schedule. \"\n",
        "                              \"See more details at \"\n",
        "                              \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
        "        self._step_count += 1\n",
        "\n",
        "        class _enable_get_lr_call:\n",
        "\n",
        "            def __init__(self, o):\n",
        "                self.o = o\n",
        "\n",
        "            def __enter__(self):\n",
        "                self.o._get_lr_called_within_step = True\n",
        "                return self\n",
        "\n",
        "            def __exit__(self, type, value, traceback):\n",
        "                self.o._get_lr_called_within_step = False\n",
        "\n",
        "        with _enable_get_lr_call(self):\n",
        "            if epoch is None:\n",
        "                self.last_epoch += 1\n",
        "                values = self.get_lr()\n",
        "            else:\n",
        "                warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
        "                self.last_epoch = epoch\n",
        "                if hasattr(self, \"_get_closed_form_lr\"):\n",
        "                    values = self._get_closed_form_lr()\n",
        "                else:\n",
        "                    values = self.get_lr()\n",
        "\n",
        "        for i, data in enumerate(zip(self.optimizer.param_groups, values)):\n",
        "            param_group, lr = data\n",
        "            param_group['lr'] = lr\n",
        "            self.print_lr(self.verbose, i, lr, epoch)\n",
        "\n",
        "        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n",
        "\n",
        "class CyclicLRGiselt_Denis(_LRSchedulerGiselt_Denis):\n",
        "    r\"\"\"Sets the learning rate of each parameter group according to\n",
        "    cyclical learning rate policy (CLR). The policy cycles the learning\n",
        "    rate between two boundaries with a constant frequency, as detailed in\n",
        "    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n",
        "    The distance between the two boundaries can be scaled on a per-iteration\n",
        "    or per-cycle basis.\n",
        "\n",
        "    Cyclical learning rate policy changes the learning rate after every batch.\n",
        "    `step` should be called after a batch has been used for training.\n",
        "\n",
        "    This class has three built-in policies, as put forth in the paper:\n",
        "\n",
        "    * \"triangular\": A basic triangular cycle without amplitude scaling.\n",
        "    * \"triangular2\": A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    * \"exp_range\": A cycle that scales initial amplitude by :math:`\\text{gamma}^{\\text{cycle iterations}}`\n",
        "      at each cycle iteration.\n",
        "\n",
        "    This implementation was adapted from the github repo: `bckenstler/CLR`_\n",
        "\n",
        "    Args:\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        base_lr (float or list): Initial learning rate which is the\n",
        "            lower boundary in the cycle for each parameter group.\n",
        "        max_lr (float or list): Upper learning rate boundaries in the cycle\n",
        "            for each parameter group. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore\n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size_up (int): Number of training iterations in the\n",
        "            increasing half of a cycle. Default: 2000\n",
        "        step_size_down (int): Number of training iterations in the\n",
        "            decreasing half of a cycle. If step_size_down is None,\n",
        "            it is set to step_size_up. Default: None\n",
        "        mode (str): One of {triangular, triangular2, exp_range}.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "            Default: 'triangular'\n",
        "        gamma (float): Constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "            Default: 1.0\n",
        "        scale_fn (function): Custom scaling policy defined by a single\n",
        "            argument lambda function, where\n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            If specified, then 'mode' is ignored.\n",
        "            Default: None\n",
        "        scale_mode (str): {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on\n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle).\n",
        "            Default: 'cycle'\n",
        "        cycle_momentum (bool): If ``True``, momentum is cycled inversely\n",
        "            to learning rate between 'base_momentum' and 'max_momentum'.\n",
        "            Default: True\n",
        "        base_momentum (float or list): Lower momentum boundaries in the cycle\n",
        "            for each parameter group. Note that momentum is cycled inversely\n",
        "            to learning rate; at the peak of a cycle, momentum is\n",
        "            'base_momentum' and learning rate is 'max_lr'.\n",
        "            Default: 0.8\n",
        "        max_momentum (float or list): Upper momentum boundaries in the cycle\n",
        "            for each parameter group. Functionally,\n",
        "            it defines the cycle amplitude (max_momentum - base_momentum).\n",
        "            The momentum at any cycle is the difference of max_momentum\n",
        "            and some scaling of the amplitude; therefore\n",
        "            base_momentum may not actually be reached depending on\n",
        "            scaling function. Note that momentum is cycled inversely\n",
        "            to learning rate; at the start of a cycle, momentum is 'max_momentum'\n",
        "            and learning rate is 'base_lr'\n",
        "            Default: 0.9\n",
        "        last_epoch (int): The index of the last batch. This parameter is used when\n",
        "            resuming a training job. Since `step()` should be invoked after each\n",
        "            batch instead of after each epoch, this number represents the total\n",
        "            number of *batches* computed, not the total number of epochs computed.\n",
        "            When last_epoch=-1, the schedule is started from the beginning.\n",
        "            Default: -1\n",
        "        verbose (bool): If ``True``, prints a message to stdout for\n",
        "            each update. Default: ``False``.\n",
        "\n",
        "    Example:\n",
        "        >>> # xdoctest: +SKIP\n",
        "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "        >>> scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
        "        >>> data_loader = torch.utils.data.DataLoader(...)\n",
        "        >>> for epoch in range(10):\n",
        "        >>>     for batch in data_loader:\n",
        "        >>>         train_batch(...)\n",
        "        >>>         scheduler.step()\n",
        "\n",
        "\n",
        "    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
        "    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 optimizer,\n",
        "                 base_lr,\n",
        "                 max_lr,\n",
        "                 step_size_up=2000,\n",
        "                 step_size_down=None,\n",
        "                 mode='triangular',\n",
        "                 gamma=1.,\n",
        "                 scale_fn=None,\n",
        "                 scale_mode='cycle',\n",
        "                 cycle_momentum=True,\n",
        "                 base_momentum=0.8,\n",
        "                 max_momentum=0.9,\n",
        "                 last_epoch=-1,\n",
        "                 verbose=False):\n",
        "\n",
        "        # Attach optimizer\n",
        "        if not isinstance(optimizer, Optimizer):\n",
        "            raise TypeError('{} is not an Optimizer'.format(\n",
        "                type(optimizer).__name__))\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "        self.direction_up = True\n",
        "        self.half_cycle_steps = 0\n",
        "\n",
        "        base_lrs = self._format_param('base_lr', optimizer, base_lr)\n",
        "        if last_epoch == -1:\n",
        "            for lr, group in zip(base_lrs, optimizer.param_groups):\n",
        "                group['lr'] = lr\n",
        "\n",
        "        self.max_lrs = self._format_param('max_lr', optimizer, max_lr)\n",
        "\n",
        "        self.step_size_up = float(step_size_up)\n",
        "        self.step_size_down = float(step_size_down) if step_size_down is not None else step_size_up\n",
        "        self.total_size = self.step_size_up + self.step_size_down\n",
        "        self.step_ratio = step_size_up / self.total_size\n",
        "\n",
        "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
        "                and scale_fn is None:\n",
        "            raise ValueError('mode is invalid and scale_fn is None')\n",
        "\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "\n",
        "        if scale_fn is None:\n",
        "            self._scale_fn_custom = None\n",
        "            if self.mode == 'triangular':\n",
        "                self._scale_fn_ref = weakref.WeakMethod(self._triangular_scale_fn)\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self._scale_fn_ref = weakref.WeakMethod(self._triangular2_scale_fn)\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self._scale_fn_ref = weakref.WeakMethod(self._exp_range_scale_fn)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self._scale_fn_custom = scale_fn\n",
        "            self._scale_fn_ref = None\n",
        "            self.scale_mode = scale_mode\n",
        "\n",
        "        self.cycle_momentum = cycle_momentum\n",
        "        if cycle_momentum:\n",
        "            if 'momentum' not in optimizer.defaults:\n",
        "                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n",
        "\n",
        "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
        "            if last_epoch == -1:\n",
        "                for momentum, group in zip(base_momentums, optimizer.param_groups):\n",
        "                    group['momentum'] = momentum\n",
        "            self.base_momentums = [group['momentum'] for group in optimizer.param_groups]\n",
        "            self.max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
        "\n",
        "        super(CyclicLRGiselt_Denis, self).__init__(optimizer, last_epoch, verbose)\n",
        "        self.base_lrs = base_lrs\n",
        "\n",
        "    def _format_param(self, name, optimizer, param):\n",
        "        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n",
        "        if isinstance(param, (list, tuple)):\n",
        "            if len(param) != len(optimizer.param_groups):\n",
        "                raise ValueError(\"expected {} values for {}, got {}\".format(\n",
        "                    len(optimizer.param_groups), name, len(param)))\n",
        "            return param\n",
        "        else:\n",
        "            return [param] * len(optimizer.param_groups)\n",
        "\n",
        "    def scale_fn(self, x):\n",
        "        if self._scale_fn_custom is not None:\n",
        "            # print(self.half_cycle_steps, x)\n",
        "            return self._scale_fn_custom(x)\n",
        "\n",
        "        else:\n",
        "            return self._scale_fn_ref()(x)\n",
        "\n",
        "    def scale_fn_rand(self, x, y):\n",
        "        return random.uniform(x, y)\n",
        "\n",
        "    def _triangular_scale_fn(self, x):\n",
        "        return 1.\n",
        "\n",
        "    def _triangular2_scale_fn(self, x):\n",
        "        return 1 / (2. ** (x - 1))\n",
        "\n",
        "    def _exp_range_scale_fn(self, x):\n",
        "        return self.gamma**(x)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"Calculates the learning rate at batch index. This function treats\n",
        "        `self.last_epoch` as the last batch index.\n",
        "\n",
        "        If `self.cycle_momentum` is ``True``, this function has a side effect of\n",
        "        updating the optimizer's momentum.\n",
        "        \"\"\"\n",
        "        \n",
        "\n",
        "        if not self._get_lr_called_within_step:\n",
        "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
        "                          \"please use `get_last_lr()`.\", UserWarning)\n",
        "\n",
        "        cycle = math.floor(1 + self.last_epoch / self.total_size)\n",
        "        x = 1. + self.last_epoch / self.total_size - cycle\n",
        "        if x <= self.step_ratio:\n",
        "            scale_factor = x / self.step_ratio\n",
        "        else:\n",
        "            scale_factor = (x - 1) / (self.step_ratio - 1)\n",
        "\n",
        "        lrs = []\n",
        "        for base_lr, max_lr in zip(self.base_lrs, self.max_lrs):\n",
        "            base_height = (max_lr - base_lr) * scale_factor\n",
        "            if self.scale_mode == 'cycle':\n",
        "\n",
        "                # print(\"cycle\")\n",
        "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
        "            elif self.scale_mode == 'iterations':\n",
        "\n",
        "                # print(\"iterations\")\n",
        "                lr = base_lr + base_height * self.scale_fn(self.last_epoch)\n",
        "\n",
        "            elif self.scale_mode == 'decrecimiento':\n",
        "                if (self.last_epoch == 0):\n",
        "                  lr = max_lr\n",
        "                else: \n",
        "                  lr = self.get_last_lr()[0] - base_lr\n",
        "                  if (lr<0):\n",
        "                    lr = base_lr\n",
        "                    \n",
        "            elif self.scale_mode == 'chipichipi':                               #<--------------------------\n",
        "                \n",
        "                '''\n",
        "                for x in optimizer.param_groups:\n",
        "                  for each in x['params']:\n",
        "                    print(each)\n",
        "                \n",
        "                for name, param in modelBB.named_parameters():\n",
        "                  print(param.grad.shape)\n",
        "                '''\n",
        "\n",
        "                # print(\"chipichipi\")\n",
        "                if (self.last_epoch == 0):\n",
        "                    last_lr = base_lr\n",
        "                else:\n",
        "                    last_lr = self.get_last_lr()[0]\n",
        "                    # print(f\"Esto es el ultimo lr {last_lr}\")\n",
        "\n",
        "                if (self.half_cycle_steps == self.step_size_up\n",
        "                    and self.direction_up == True):\n",
        "                    #print('Cambio de direccion: bajando')\n",
        "                    lr = random.uniform(last_lr, max_lr)\n",
        "                    self.direction_up = False\n",
        "                    self.half_cycle_steps = 1\n",
        "\n",
        "                elif (self.half_cycle_steps == self.step_size_down\n",
        "                    and self.direction_up == False):\n",
        "                    #print('Cambio de direccion: subiendo')\n",
        "                    lr = random.uniform(base_lr, last_lr)\n",
        "                    self.direction_up = True\n",
        "                    self.half_cycle_steps = 1\n",
        "\n",
        "                elif (self.direction_up == True):\n",
        "                    lr = random.uniform(last_lr, max_lr)\n",
        "                    self.half_cycle_steps += 1\n",
        "\n",
        "                elif(self.direction_up == False):\n",
        "                    lr = random.uniform(base_lr, last_lr)\n",
        "                    self.half_cycle_steps += 1\n",
        "            #print('base_lr: '+str(base_lr))\n",
        "            lrs.append(lr)\n",
        "\n",
        "        if self.cycle_momentum:\n",
        "            #print('wowo')\n",
        "            momentums = []\n",
        "            for base_momentum, max_momentum in zip(self.base_momentums, self.max_momentums):\n",
        "                base_height = (max_momentum - base_momentum) * scale_factor\n",
        "                if self.scale_mode == 'cycle':\n",
        "                    momentum = max_momentum - base_height * self.scale_fn(cycle)\n",
        "                else:\n",
        "                    momentum = max_momentum - base_height * self.scale_fn(self.last_epoch)\n",
        "                #print('momentum: '+str(max_momentum)+' '+str(base_momentum))\n",
        "                momentums.append(momentum)\n",
        "            for param_group, momentum in zip(self.optimizer.param_groups, momentums):\n",
        "                param_group['momentum'] = momentum\n",
        "            \n",
        "        return lrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f8ZZZ1Hcg2R"
      },
      "source": [
        "## Random Cyclic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AG-ZFJ9fGaW"
      },
      "outputs": [],
      "source": [
        "def CyclicGD():\n",
        "    import random\n",
        "    modelRandomCyclic = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                        nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                        nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelRandomCyclic.parameters(), lr=lr)\n",
        "\n",
        "    clr_fn = lambda x, y: random.uniform(x, y)\n",
        "\n",
        "    scheduler = CyclicLRGiselt_Denis(optimizer, base_lr=0.01, max_lr=0.1,step_size_up=3, scale_fn=clr_fn,scale_mode='chipichipi', cycle_momentum=False)\n",
        "\n",
        "    start.record()\n",
        "    random_cyclic_acc_list, random_cyclic_cost_list, random_cyclic_lr_list, random_cyclic_epochs= train(modelRandomCyclic, optimizer,scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize() \n",
        "    random_cyclic_time = start.elapsed_time(end)\n",
        "\n",
        "    random_cyclic_acc = accuracy(modelRandomCyclic, x_test_tensor,  y_test_tensor, mb_size) \n",
        "\n",
        "    return random_cyclic_acc_list, random_cyclic_cost_list, random_cyclic_lr_list, random_cyclic_time, random_cyclic_acc, random_cyclic_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acBgxgGkZdL6",
        "outputId": "7dc5d4b8-8968-40a2-c19e-a7f0a3bf8953"
      },
      "outputs": [],
      "source": [
        "resultados['random_cyclic'] = {}\n",
        "resultados['random_cyclic']['val_acc_list'] = [0] * epochs\n",
        "resultados['random_cyclic']['test_acc'] = 0\n",
        "resultados['random_cyclic']['cost'] = [0] * epochs\n",
        "resultados['random_cyclic']['time'] = 0\n",
        "resultados['random_cyclic']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    random_cyclic_acc_list, random_cyclic_cost_list, random_cyclic_lr_list, random_cyclic_time, random_cyclic_acc, random_cyclic_epochs = CyclicGD()\n",
        "    a = SumList(resultados['random_cyclic']['val_acc_list'], random_cyclic_acc_list)\n",
        "    resultados['random_cyclic']['val_acc_list'] = SumList(resultados['random_cyclic']['val_acc_list'], random_cyclic_acc_list)\n",
        "    resultados['random_cyclic']['test_acc'] += random_cyclic_acc\n",
        "    resultados['random_cyclic']['cost'] = SumList(resultados['random_cyclic']['cost'], random_cyclic_cost_list)\n",
        "    resultados['random_cyclic']['time'] += random_cyclic_time\n",
        "    resultados['random_cyclic']['epochs'] += random_cyclic_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['random_cyclic']['name'] = 'Random Ciclico'\n",
        "resultados['random_cyclic']['lr'] = random_cyclic_lr_list\n",
        "resultados['random_cyclic']['test_acc'] = resultados['random_cyclic']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['random_cyclic']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['random_cyclic']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['random_cyclic']['cost'] = DeleteZerosFromList(DivideList(resultados['random_cyclic']['cost'], MAX_ITERATIONS))\n",
        "resultados['random_cyclic']['time'] = resultados['random_cyclic']['time']\n",
        "resultados['random_cyclic']['epochs'] = resultados['random_cyclic']['epochs'] / MAX_ITERATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dD4C1mSZidw"
      },
      "source": [
        "## Nuestro decreciente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-104rC6UfGaX"
      },
      "outputs": [],
      "source": [
        "def Our_Decay():\n",
        "    modelOurDecay = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimizer = torch.optim.SGD(modelOurDecay.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "    clr_fn = lambda x, y: random.uniform(x, y)\n",
        "    scheduler = CyclicLRGiselt_Denis(optimizer, base_lr=0.0001, max_lr=0.1,scale_fn=clr_fn, step_size_up=1,scale_mode='decrecimiento', cycle_momentum=False)\n",
        "\n",
        "    start.record()\n",
        "    our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_epochs = train(modelOurDecay,optimizer, scheduler, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    our_decay_time = start.elapsed_time(end)\n",
        "\n",
        "    our_decay_acc = accuracy(modelOurDecay, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_time, our_decay_acc, our_decay_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1gWBYUD-F8g",
        "outputId": "ddc26a38-5278-423b-dccf-0c7b86273082"
      },
      "outputs": [],
      "source": [
        "resultados['our_decay'] = {}\n",
        "resultados['our_decay']['val_acc_list'] = [0] * epochs\n",
        "resultados['our_decay']['test_acc'] = 0\n",
        "resultados['our_decay']['cost'] = [0] * epochs\n",
        "resultados['our_decay']['time'] = 0\n",
        "resultados['our_decay']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    our_decay_acc_list, our_decay_cost_list, our_decay_lr_list, our_decay_time, our_decay_acc, our_decay_epochs = Our_Decay()\n",
        "    resultados['our_decay']['val_acc_list'] = SumList(resultados['our_decay']['val_acc_list'], our_decay_acc_list)\n",
        "    resultados['our_decay']['test_acc'] += our_decay_acc\n",
        "    resultados['our_decay']['cost'] = SumList(resultados['our_decay']['cost'], our_decay_cost_list)\n",
        "    resultados['our_decay']['time'] += our_decay_time\n",
        "    resultados['our_decay']['epochs'] += our_decay_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['our_decay']['name'] = 'Our Decay'\n",
        "resultados['our_decay']['lr'] = our_decay_lr_list\n",
        "resultados['our_decay']['test_acc'] = resultados['our_decay']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['our_decay']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['our_decay']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['our_decay']['cost'] = DeleteZerosFromList(DivideList(resultados['our_decay']['cost'], MAX_ITERATIONS))\n",
        "resultados['our_decay']['time'] = resultados['our_decay']['time'] \n",
        "resultados['our_decay']['epochs'] = resultados['our_decay']['epochs'] / MAX_ITERATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqmMCfIAHpgs"
      },
      "source": [
        "## L-BFGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rzU1bVRfGaY"
      },
      "outputs": [],
      "source": [
        "def LBFGS():\n",
        "    modelLBFGS = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "\n",
        "    optimizer = torch.optim.LBFGS(modelLBFGS.parameters(),\n",
        "                                lr=1,\n",
        "                                history_size=10, #update history size. What's this?\n",
        "                                max_iter=1, #maximal number of iterations per optimization step\n",
        "                                )\n",
        "\n",
        "    lbfgs_cost_list = [0.0]\n",
        "    lbfgs_acc_list = [0.0]\n",
        "    modelLBFGS = modelLBFGS.to(device=device)\n",
        "    x_train_tensor_ = x_train_tensor.to(device=device, dtype=torch.float32)\n",
        "    y_train_tensor_ = y_train_tensor.to(device=device, dtype=torch.long)\n",
        "    i = 0\n",
        "\n",
        "    start.record()\n",
        "    #training\n",
        "    while (lbfgs_acc_list[-1] < 0.95):\n",
        "        #print('Iteracion: '+ str(i))\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            scores = modelLBFGS(x_train_tensor_)\n",
        "            cost = F.cross_entropy(input= scores, target=y_train_tensor_.squeeze())\n",
        "            cost.backward()\n",
        "            print(f'costo: {cost.item()}')\n",
        "            return cost\n",
        "        cost = optimizer.step(closure)\n",
        "        lbfgs_cost_list.append(cost.item())\n",
        "        lbfgs_acc_list.append(accuracy(modelLBFGS, x_val_tensor, y_val_tensor, mb_size))\n",
        "        #print(f'accuracy: {lbfgs_acc_list[-1]}')\n",
        "        i+=1\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    lbfgs_time = start.elapsed_time(end)\n",
        "\n",
        "    lbfgs_acc = accuracy(modelLBFGS, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return lbfgs_acc_list, lbfgs_cost_list, [0] ,lbfgs_time, lbfgs_acc, i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJSxQ5xeHu2U",
        "outputId": "3d89aa0b-3294-4cda-83d6-3c548f45ed73"
      },
      "outputs": [],
      "source": [
        "resultados['lbfgs'] = {}\n",
        "resultados['lbfgs']['val_acc_list'] = [0] * epochs\n",
        "resultados['lbfgs']['test_acc'] = 0\n",
        "resultados['lbfgs']['cost'] = [0] * epochs\n",
        "resultados['lbfgs']['time'] = 0\n",
        "resultados['lbfgs']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    lbfgs_acc_list, lbfgs_cost_list, lbfgs_lr_list, lbfgs_time, lbfgs_acc, lbfgs_epochs = LBFGS()\n",
        "    resultados['lbfgs']['val_acc_list'] = SumList(resultados['lbfgs']['val_acc_list'], lbfgs_acc_list)\n",
        "    resultados['lbfgs']['test_acc'] += lbfgs_acc\n",
        "    resultados['lbfgs']['cost'] = SumList(resultados['lbfgs']['cost'], lbfgs_cost_list)\n",
        "    resultados['lbfgs']['time'] += lbfgs_time\n",
        "    resultados['lbfgs']['epochs'] += lbfgs_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['lbfgs']['name'] = 'LBFGS'\n",
        "resultados['lbfgs']['lr'] = lbfgs_lr_list\n",
        "resultados['lbfgs']['test_acc'] = resultados['lbfgs']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['lbfgs']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['lbfgs']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['lbfgs']['cost'] = DeleteZerosFromList(DivideList(resultados['lbfgs']['cost'], MAX_ITERATIONS))\n",
        "resultados['lbfgs']['time'] = resultados['lbfgs']['time'] \n",
        "resultados['lbfgs']['epochs'] = resultados['lbfgs']['epochs'] / MAX_ITERATIONS\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jQLmr_cd9D8"
      },
      "source": [
        "## L-BFGS with line search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiXaFYWVfGaZ"
      },
      "outputs": [],
      "source": [
        "def LBFGS_LS():\n",
        "    modelLBFGS_LS = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "\n",
        "    optimizer = torch.optim.LBFGS(modelLBFGS_LS.parameters(),\n",
        "                                lr=1,\n",
        "                                history_size=10, #update history size. What's this?\n",
        "                                #max_eval (int) ‚Äì maximal number of function evaluations per optimization step (default: max_iter * 1.25)\n",
        "                                #max_iter=1,\n",
        "                                line_search_fn=\"strong_wolfe\"\n",
        "                                )\n",
        "\n",
        "    lbfgs_ls_cost_list = [0.0]\n",
        "    lbfgs_ls_acc_list = [0.0]\n",
        "    modelLBFGS_LS = modelLBFGS_LS.to(device=device)\n",
        "    x_train_tensor_ = x_train_tensor.to(device=device, dtype=torch.float32)\n",
        "    y_train_tensor_ = y_train_tensor.to(device=device, dtype=torch.long)\n",
        "    i = 0\n",
        "\n",
        "    start.record()\n",
        "    #training\n",
        "    while (lbfgs_ls_acc_list[-1] < 0.95):\n",
        "        print('Iteracion: '+ str(i))\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            scores = modelLBFGS_LS(x_train_tensor_)\n",
        "            cost = F.cross_entropy(input= scores, target=y_train_tensor_.squeeze())\n",
        "            cost.backward()\n",
        "            print(f'costo: {cost.item()}')  \n",
        "            lbfgs_ls_cost_list.append(cost.item())\n",
        "            return cost\n",
        "        optimizer.step(closure)\n",
        "        lbfgs_ls_acc_list.append(accuracy(modelLBFGS_LS, x_val_tensor, y_val_tensor, mb_size))\n",
        "        i+=1\n",
        "            \n",
        "    #print(f'accuracy: {lbfgs_ls_acc_list[-1]}')\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    lbfgs_ls_time = start.elapsed_time(end)\n",
        "\n",
        "    lbfgs_ls_acc = accuracy(modelLBFGS_LS, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return lbfgs_ls_acc_list, lbfgs_ls_cost_list, [0] ,lbfgs_ls_time, lbfgs_ls_acc, i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oofvD4gNeC4t",
        "outputId": "d220d54e-c4bb-4892-dd77-85b1ada3346e"
      },
      "outputs": [],
      "source": [
        "resultados['lbfgs_ls'] = {}\n",
        "resultados['lbfgs_ls']['val_acc_list'] = [0] * epochs\n",
        "resultados['lbfgs_ls']['test_acc'] = 0\n",
        "resultados['lbfgs_ls']['cost'] = [0] * epochs\n",
        "resultados['lbfgs_ls']['time'] = 0\n",
        "resultados['lbfgs_ls']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    lbfgs_ls_acc_list, lbfgs_ls_cost_list, lbfgs_ls_lr_list ,lbfgs_ls_time, lbfgs_ls_acc, lbfgs_ls_epochs = LBFGS_LS()\n",
        "    resultados['lbfgs_ls']['val_acc_list'] = SumList(resultados['lbfgs_ls']['val_acc_list'], lbfgs_ls_acc_list)\n",
        "    resultados['lbfgs_ls']['test_acc'] += lbfgs_ls_acc\n",
        "    resultados['lbfgs_ls']['cost'] = SumList(resultados['lbfgs_ls']['cost'], lbfgs_ls_cost_list)\n",
        "    resultados['lbfgs_ls']['time'] += lbfgs_ls_time\n",
        "    resultados['lbfgs_ls']['epochs'] += lbfgs_ls_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['lbfgs_ls']['name'] = 'LBFGS With LS'\n",
        "resultados['lbfgs_ls']['lr'] = lbfgs_ls_lr_list\n",
        "resultados['lbfgs_ls']['test_acc'] = resultados['lbfgs_ls']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['lbfgs_ls']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['lbfgs_ls']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['lbfgs_ls']['cost'] = DeleteZerosFromList(DivideList(resultados['lbfgs_ls']['cost'], MAX_ITERATIONS))\n",
        "resultados['lbfgs_ls']['time'] = resultados['lbfgs_ls']['time'] \n",
        "resultados['lbfgs_ls']['epochs'] = resultados['lbfgs_ls']['epochs'] / MAX_ITERATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhXFwQqymLvv"
      },
      "source": [
        "## Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z9uCM6afGaa"
      },
      "outputs": [],
      "source": [
        "def Adam():\n",
        "    modelAdam = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                        nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                        nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimiserAdam = torch.optim.Adam(modelAdam.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "    start.record()\n",
        "    adam_acc_list, adam_cost_list,adam_lr_list, adam_epochs = train(modelAdam,optimiserAdam,None, mb_size)\n",
        "    end.record()\n",
        "\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    adam_time = start.elapsed_time(end)\n",
        "\n",
        "    adam_acc = accuracy(modelAdam, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return adam_acc_list, adam_cost_list, adam_lr_list, adam_time, adam_acc, adam_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZeKlCWWmNpH",
        "outputId": "dc73bfc6-dd5f-45f1-a8a0-084ca599ce6d"
      },
      "outputs": [],
      "source": [
        "\n",
        "resultados['adam'] = {}\n",
        "resultados['adam']['val_acc_list'] = [0] * epochs\n",
        "resultados['adam']['test_acc'] = 0\n",
        "resultados['adam']['cost'] = [0] * epochs\n",
        "resultados['adam']['time'] = 0\n",
        "resultados['adam']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    adam_acc_list, adam_cost_list, adam_lr_list, adam_time, adam_acc, adam_epochs = Adam()\n",
        "    resultados['adam']['val_acc_list'] = SumList(resultados['adam']['val_acc_list'], adam_acc_list)\n",
        "    resultados['adam']['test_acc'] += adam_acc\n",
        "    resultados['adam']['cost'] = SumList(resultados['adam']['cost'], adam_cost_list)\n",
        "    resultados['adam']['time'] += adam_time\n",
        "    resultados['adam']['epochs'] += adam_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['adam']['name'] = 'Adam'\n",
        "resultados['adam']['lr'] = adam_lr_list\n",
        "resultados['adam']['test_acc'] = resultados['adam']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['adam']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['adam']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['adam']['cost'] = DeleteZerosFromList(DivideList(resultados['adam']['cost'], MAX_ITERATIONS))\n",
        "resultados['adam']['time'] = resultados['adam']['time'] \n",
        "resultados['adam']['epochs'] = resultados['adam']['epochs'] / MAX_ITERATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzehV-6emhDY"
      },
      "source": [
        "## Momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WhsNUpTfGab"
      },
      "outputs": [],
      "source": [
        "def SGDM():\n",
        "    modelSGDM = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimiserSGDM = torch.optim.SGD(modelSGDM.parameters(), lr=lr, momentum=0.9)\n",
        "    start.record()\n",
        "    SGDM_acc_list, SGDM_cost_list,SGDM_lr_list, SGDM_epochs = train(modelSGDM, optimiserSGDM,None, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    SGDM_time = start.elapsed_time(end)\n",
        "\n",
        "    SGDM_acc = accuracy(modelSGDM, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return SGDM_acc_list, SGDM_cost_list, SGDM_lr_list, SGDM_time, SGDM_acc, SGDM_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4bdgJL-mkx3",
        "outputId": "895bed37-604b-40c5-c349-d6a7dbf98c64"
      },
      "outputs": [],
      "source": [
        "resultados['SGDM'] = {}\n",
        "resultados['SGDM']['val_acc_list'] = [0] * epochs\n",
        "resultados['SGDM']['test_acc'] = 0\n",
        "resultados['SGDM']['cost'] = [0] * epochs\n",
        "resultados['SGDM']['time'] = 0\n",
        "resultados['SGDM']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    SGDM_acc_list, SGDM_cost_list, SGDM_lr_list, SGDM_time, SGDM_acc, SGDM_epochs = SGDM()\n",
        "    resultados['SGDM']['val_acc_list'] = SumList(resultados['SGDM']['val_acc_list'], SGDM_acc_list)\n",
        "    resultados['SGDM']['test_acc'] += SGDM_acc\n",
        "    resultados['SGDM']['cost'] = SumList(resultados['SGDM']['cost'], SGDM_cost_list)\n",
        "    resultados['SGDM']['time'] += SGDM_time\n",
        "    resultados['SGDM']['epochs'] += SGDM_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['SGDM']['name'] = 'SGDM'\n",
        "resultados['SGDM']['lr'] = SGDM_lr_list\n",
        "resultados['SGDM']['test_acc'] = resultados['SGDM']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['SGDM']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['SGDM']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['SGDM']['cost'] = DeleteZerosFromList(DivideList(resultados['SGDM']['cost'], MAX_ITERATIONS))\n",
        "resultados['SGDM']['time'] = resultados['SGDM']['time'] \n",
        "resultados['SGDM']['epochs'] = resultados['SGDM']['epochs'] / MAX_ITERATIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb5Poi8cmVHP"
      },
      "source": [
        "## RMSProp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEqYnO8vfGac"
      },
      "outputs": [],
      "source": [
        "def RMSP():\n",
        "    modelRMSP = nn.Sequential(nn.Linear(in_features=input_layer, out_features=layer1), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer1, out_features=layer2), nn.ReLU(),\n",
        "                       nn.Linear(in_features=layer2, out_features=10))\n",
        "    optimiserRMSP = torch.optim.RMSprop(modelRMSP.parameters(), lr=lr, alpha=0.9)\n",
        "    start.record()\n",
        "    RMSP_acc_list, RMSP_cost_list,RMSP_lr_list, RMSP_epochs = train(modelRMSP, optimiserRMSP,None, mb_size)\n",
        "    end.record()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    RMSP_time = start.elapsed_time(end)\n",
        "\n",
        "    RMSP_acc = accuracy(modelRMSP, x_test_tensor,  y_test_tensor, mb_size)\n",
        "\n",
        "    return RMSP_acc_list, RMSP_cost_list, RMSP_lr_list, RMSP_time, RMSP_acc, RMSP_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv7uCfwOmXN3",
        "outputId": "c95bbd2b-c51b-4b0e-9a30-67a8bd398e5b"
      },
      "outputs": [],
      "source": [
        "resultados['RMSP'] = {}\n",
        "resultados['RMSP']['val_acc_list'] = [0] * epochs\n",
        "resultados['RMSP']['test_acc'] = 0\n",
        "resultados['RMSP']['cost'] = [0] * epochs\n",
        "resultados['RMSP']['time'] = 0\n",
        "resultados['RMSP']['epochs'] = 0\n",
        "\n",
        "for _ in range(MAX_ITERATIONS):\n",
        "    RMSP_acc_list, RMSP_cost_list, RMSP_lr_list, RMSP_time, RMSP_acc, RMSP_epochs = SGDM()\n",
        "    resultados['RMSP']['val_acc_list'] = SumList(resultados['RMSP']['val_acc_list'], RMSP_acc_list)\n",
        "    resultados['RMSP']['test_acc'] += RMSP_acc\n",
        "    resultados['RMSP']['cost'] = SumList(resultados['RMSP']['cost'], RMSP_cost_list)\n",
        "    resultados['RMSP']['time'] += RMSP_time\n",
        "    resultados['RMSP']['epochs'] += RMSP_epochs\n",
        "\n",
        "#Saving results\n",
        "resultados['RMSP']['name'] = 'RMSP'\n",
        "resultados['RMSP']['lr'] = RMSP_lr_list\n",
        "resultados['RMSP']['test_acc'] = resultados['RMSP']['test_acc'] / MAX_ITERATIONS\n",
        "resultados['RMSP']['val_acc_list'] = DeleteZerosFromList(DivideList(resultados['RMSP']['val_acc_list'], MAX_ITERATIONS))\n",
        "resultados['RMSP']['cost'] = DeleteZerosFromList(DivideList(resultados['RMSP']['cost'], MAX_ITERATIONS))\n",
        "resultados['RMSP']['time'] = resultados['RMSP']['time'] \n",
        "resultados['RMSP']['epochs'] = resultados['RMSP']['epochs'] / MAX_ITERATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8_h21aMdgsB"
      },
      "source": [
        "# Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJQ2_Q_q-s3a"
      },
      "source": [
        "Al guardar todos los resultados de cada m√©todo en sus respectivos diccionarios, se procede a crear un dataframe con estos resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for key, _ in resultados.items():\n",
        "    if( resultados[key]['val_acc_list'][0] == 0 ):\n",
        "        continue\n",
        "    resultados[key]['val_acc_list'].insert(0,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S5hiFtm-9xj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "resultados_df = pd.DataFrame(resultados.copy()).T\n",
        "\n",
        "resultados_df['val_acc'] = resultados_df.apply(lambda row: round(row['val_acc_list'][-1]* 100,2), axis=1)\n",
        "resultados_df['test_acc'] = resultados_df.apply(lambda row: round(row['test_acc']*100,2), axis=1)\n",
        "# resultados_df['epochs'] = resultados_df.apply(lambda row: len(row['val_acc_list']), axis=1)\n",
        "resultados_df['time'] = resultados_df.apply(lambda row: round(row['time']/(1000),2), axis=1)\n",
        "resultados_df['val_acc'] = resultados_df.apply(lambda row: '{acc}%'.format(acc = row['val_acc']), axis=1)\n",
        "resultados_df['test_acc'] = resultados_df.apply(lambda row: '{acc}%'.format(acc = row['test_acc']), axis=1)\n",
        "\n",
        "resultados_df = resultados_df.sort_values(by=['epochs'],ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(resultados['lbfgs_ls']['epochs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "I_97z5Ll7cX2",
        "outputId": "63c03eb7-a621-447f-dbc8-24d4260d01b7"
      },
      "outputs": [],
      "source": [
        "resultados_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq4MOLZidpgh"
      },
      "source": [
        "## Traza de tasas de aprendizaje no adaptativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kPz_53-BduvK",
        "outputId": "98aef29c-047d-40f0-e22f-eb4ccf753fb1"
      },
      "outputs": [],
      "source": [
        "from matplotlib.ticker import MultipleLocator\n",
        "\n",
        "fig, ax = plt.subplots(3,2,figsize=(14, 15))\n",
        "\n",
        "method = [fixed_lr_list, decay_lr_list, our_decay_lr_list,cyclic_lr_list, random_cyclic_lr_list]\n",
        "names = ['Tasa de aprendizaje constante', 'Tasa de aprendizaje dereciente (PyTorch)', 'Tasa de aprendizaje dereciente (Propio)', 'Tasa de aprendizaje c√≠clica', 'Tasa de aprendizaje c√≠clica aleatoria']\n",
        "i = 0\n",
        "for a in range(3):\n",
        "  for b in range(2):\n",
        "    if(a==2 and b==1):\n",
        "      break\n",
        "    ax[a,b].plot(range(len(method[i]))[1:], \n",
        "            method[i][1:], \n",
        "            'black',\n",
        "            marker = 'o')\n",
        "    #ax[a,b].set_xlim([1, len(method[i])])\n",
        "    #ax[a,b].set_ylim([0, max(method[i]) + 0.01])\n",
        "    ax[a,b].set_xlabel('# Epochs') #, fontsize = 18)\n",
        "    ax[a,b].set_ylabel('Tasa de aprendizaje') #, fontsize = 18)\n",
        "    ax[a,b].spines['top'].set_visible(False)\n",
        "    ax[a,b].spines['right'].set_visible(False)\n",
        "    #ax[a,b].xaxis.set_major_locator(MultipleLocator(len(method[i])//4))\n",
        "    #ax[a,b].xaxis.set_minor_locator(MultipleLocator(len(method[i])//4))\n",
        "    ax[a,b].set_title(names[i])\n",
        "    i+=1\n",
        "\n",
        "\n",
        "fig.suptitle('Comportamiento de tasas de aprendizaje por epoch')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Qelt3wIs3T"
      },
      "source": [
        "## Tiempos por epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "lruyulJ5C4SR",
        "outputId": "5c187e83-54c2-446b-9904-45416b843d5d"
      },
      "outputs": [],
      "source": [
        "resultados_df[['name','val_acc','time','test_acc']].style.hide_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhAah9L2cmoj"
      },
      "source": [
        "## Convergencia en iteraciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFSasuSZ2koT"
      },
      "source": [
        "### Muestra: precisi√≥n en el conjunto de validaci√≥n y prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "bP3C7xC1vql8",
        "outputId": "b0521c8b-9eca-4f84-8511-090a205272e9"
      },
      "outputs": [],
      "source": [
        "from matplotlib.ticker import MultipleLocator\n",
        "\n",
        "bbox = dict(boxstyle =\"round\", fc =\"1\")\n",
        "arrowprops = dict(\n",
        "    arrowstyle = \"->\")\n",
        "offset = 72\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "\n",
        "for k1 in resultados_df.index:\n",
        "    ax.plot(range(len(resultados_df.loc[k1,'val_acc_list'])),\n",
        "        resultados_df.loc[k1,'val_acc_list'],\n",
        "        label='{name} = {acc}%'.format(name= resultados_df.loc[k1,'name'],\n",
        "                                       acc = round(resultados_df.loc[k1,'val_acc_list'][-1] * 100 , 2)))\n",
        "\n",
        "plt.title('Epochs vs Accuracy Train Set', fontsize = 18)\n",
        "\n",
        "#ax.set_xlim([0, 50])\n",
        "#ax.set_ylim([0, 1])\n",
        "\n",
        "ax.set_xlabel('# Epochs', fontsize = 18)\n",
        "ax.set_ylabel('Train Accuracy', fontsize = 18)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.xaxis.set_major_locator(MultipleLocator(10))\n",
        "ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
        "ax.legend()\n",
        "\n",
        "# ax.annotate('data = (%.1f, %.1f)'%(14, 0.95),\n",
        "#             (14, 0.95), xytext =(1 + 14,0.95),\n",
        "#             textcoords ='offset points',\n",
        "#             bbox = bbox, arrowprops = arrowprops)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY3jQ5IWgUu1"
      },
      "source": [
        "### Medida: costo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "244PkTh8t2hF",
        "outputId": "01dffa83-0493-4a19-c3cf-07707193b5da"
      },
      "outputs": [],
      "source": [
        "from matplotlib.ticker import MultipleLocator\n",
        "\n",
        "bbox = dict(boxstyle =\"round\", fc =\"1\")\n",
        "arrowprops = dict(\n",
        "    arrowstyle = \"->\")\n",
        "offset = 72\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "\n",
        "for k1 in resultados_df.index:\n",
        "    ax.plot(range(len(resultados_df.loc[k1,'cost'])),\n",
        "        resultados_df.loc[k1,'cost'],\n",
        "        label='{name} = {acc}%'.format(name= resultados_df.loc[k1,'name'],\n",
        "                                       acc = resultados_df.loc[k1,'val_acc']))\n",
        "\n",
        "plt.title('Epochs vs Costo', fontsize = 18)\n",
        "\n",
        "\n",
        "ax.set_xlabel('# Epochs', fontsize = 18)\n",
        "ax.set_ylabel('Costo', fontsize = 18)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.xaxis.set_major_locator(MultipleLocator(10))\n",
        "ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
        "ax.legend()\n",
        "\n",
        "# ax.annotate('data = (%.1f, %.1f)'%(14, 0.95),\n",
        "#             (14, 0.95), xytext =(1 + 14,0.95),\n",
        "#             textcoords ='offset points',\n",
        "#             bbox = bbox, arrowprops = arrowprops)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "ccfea65839ea7b4cb7611917847ae955f8a5d3f7496a05c88f3bf628abe673bf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
